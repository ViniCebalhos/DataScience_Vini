# ğŸ“Š Data Mining - Trabalhos e Projetos

Esta pasta contÃ©m todos os trabalhos desenvolvidos na disciplina de **Data Mining** da pÃ³s-graduaÃ§Ã£o em CiÃªncia de Dados.

---

## ğŸ“š Trabalhos Desenvolvidos

### ğŸ” [Work 1: Web Scraping e AnÃ¡lise de Dados do YouTube](./work1/)
**Objetivo:** Extrair e analisar dados do YouTube usando a API oficial do Google.

**Tecnologias:**
- Python (googleapiclient)
- YouTube Data API v3
- Pandas para anÃ¡lise

**ConteÃºdo:**
- ExtraÃ§Ã£o de dados de vÃ­deos do YouTube
- AnÃ¡lise de estatÃ­sticas de vÃ­deos
- Processamento de dados de API

**Arquivos principais:**
- `first.ipynb` - Notebook principal de extraÃ§Ã£o
- `third.ipynb` - AnÃ¡lise dos dados extraÃ­dos

---

### ğŸ“ˆ [Work 2: RegressÃ£o Linear e AnÃ¡lise de Dados](./work2/)
**Objetivo:** Aplicar conceitos fundamentais de dados e regressÃ£o linear em datasets reais.

**Tecnologias:**
- Python (pandas, numpy, scikit-learn)
- RegressÃ£o Linear
- AnÃ¡lise exploratÃ³ria de dados

**ConteÃºdo:**
- AnÃ¡lise do dataset Iris
- AnÃ¡lise do dataset Titanic
- RegressÃ£o linear em dados de carros
- EstatÃ­sticas descritivas

**Datasets utilizados:**
- Iris.csv
- titanic.csv
- datasetCarros.csv

**Arquivos principais:**
- `Exercicio_Conceitos_fundamentais_de_dados_e_RegressÃ£o_Linear.ipynb`
- `Exercicio_Conceitos_fundamentais_de_dados_e_RegressÃ£o_Linear2.ipynb`

---

### ğŸ¯ [Work 3: ClassificaÃ§Ã£o e Agrupamento (Clustering)](./work3/)
**Objetivo:** Aplicar tÃ©cnicas de classificaÃ§Ã£o e clustering em dados do Titanic.

**Tecnologias:**
- Python (scikit-learn)
- K-Means Clustering
- Algoritmos de ClassificaÃ§Ã£o

**ConteÃºdo:**
- Clustering de passageiros do Titanic
- AnÃ¡lise de grupos identificados
- VisualizaÃ§Ãµes de clusters

**Arquivos principais:**
- `exercicio_titanic_clustering.ipynb`
- `ExercicioClassificacao_e_Agrupamento.pdf` - Enunciado do trabalho

---

### ğŸ”— [Work 4: Regras de AssociaÃ§Ã£o e MineraÃ§Ã£o de Texto](./work4/)
**Objetivo:** Aplicar tÃ©cnicas de regras de associaÃ§Ã£o e anÃ¡lise de texto.

**Tecnologias:**
- Python (mlxtend, nltk)
- Apriori Algorithm
- Processamento de Linguagem Natural

**ConteÃºdo:**
- Regras de associaÃ§Ã£o em dados de supermercado
- AnÃ¡lise de sentimento em tweets
- MineraÃ§Ã£o de texto em dados do Twitter

**Datasets utilizados:**
- supermercado.csv
- tweets_trump.csv

**Arquivos principais:**
- `work4.ipynb` - Notebook principal
- `work4.html` - VersÃ£o HTML exportada

---

### ğŸ† [Challenge: CompetiÃ§Ã£o Kaggle](./challenge/)
**Objetivo:** Prever locais altamente avaliados em Toronto usando dados do Yelp.

**CompetiÃ§Ã£o:** Predict Highly Rated Venues CDA UTFPR 2024

**Resultados:**
- **Melhor Modelo:** Random Forest
- **F1-Score:** 0.9991
- **Score CV:** 0.9991 +/- 0.0003

**Tecnologias:**
- Python (scikit-learn, pandas, numpy)
- Random Forest Classifier
- Gradient Boosting
- Logistic Regression
- Grid Search para otimizaÃ§Ã£o

**Metodologia:**
1. AnÃ¡lise ExploratÃ³ria de Dados (EDA)
2. PrÃ©-processamento e feature engineering
3. Teste de mÃºltiplos algoritmos
4. OtimizaÃ§Ã£o de hiperparÃ¢metros
5. ValidaÃ§Ã£o cruzada

**Arquivos principais:**
- `challenge_final.ipynb` - Notebook final com melhor modelo
- `submission_best_model.csv` - SubmissÃ£o final
- MÃºltiplas submissÃµes testadas com diferentes thresholds

---

## ğŸ› ï¸ Tecnologias e Bibliotecas Utilizadas

### Principais
- **Python 3.x**
- **Pandas** - ManipulaÃ§Ã£o de dados
- **NumPy** - ComputaÃ§Ã£o numÃ©rica
- **Scikit-learn** - Machine Learning
- **Matplotlib/Seaborn** - VisualizaÃ§Ã£o

### EspecÃ­ficas
- **Google API Client** - Web scraping do YouTube
- **MLxtend** - Regras de associaÃ§Ã£o (Apriori)
- **NLTK** - Processamento de linguagem natural
- **XGBoost** - Gradient Boosting

---

## ğŸ“Š CompetÃªncias Demonstradas

- âœ… Web Scraping e APIs
- âœ… AnÃ¡lise ExploratÃ³ria de Dados (EDA)
- âœ… RegressÃ£o Linear
- âœ… ClassificaÃ§Ã£o e Clustering
- âœ… Regras de AssociaÃ§Ã£o
- âœ… MineraÃ§Ã£o de Texto
- âœ… Feature Engineering
- âœ… OtimizaÃ§Ã£o de Modelos
- âœ… ValidaÃ§Ã£o Cruzada

---

## ğŸ“ Estrutura de Pastas

```
data_mining/
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ Aula3/                       # ExercÃ­cios de aula
â”œâ”€â”€ work1/                       # Web Scraping YouTube
â”œâ”€â”€ work2/                       # RegressÃ£o Linear
â”œâ”€â”€ work3/                       # ClassificaÃ§Ã£o e Clustering
â”œâ”€â”€ work4/                       # Regras de AssociaÃ§Ã£o e Texto
â”œâ”€â”€ challenge/                   # CompetiÃ§Ã£o Kaggle
â””â”€â”€ lib/                         # Bibliotecas auxiliares
```

---

## ğŸš€ Como Executar

Cada trabalho possui seus prÃ³prios notebooks Jupyter. Para executar:

1. Instale as dependÃªncias:
```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

2. Abra o notebook desejado:
```bash
jupyter notebook work[N]/[nome_do_notebook].ipynb
```

3. Execute as cÃ©lulas sequencialmente

---

## ğŸ“ Notas

- Alguns datasets podem precisar ser baixados separadamente
- APIs podem requerer chaves de acesso (YouTube API)
- Os notebooks estÃ£o documentados em portuguÃªs
- Resultados e visualizaÃ§Ãµes sÃ£o gerados automaticamente

---

**Disciplina:** Data Mining  
**Autor:** VinÃ­cius de Souza Cebalhos  
**InstituiÃ§Ã£o:** UTFPR

