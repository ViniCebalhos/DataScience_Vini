---
marp: true
theme: default
paginate: true
---

# Transformers em Machine Learning
## An√°lise de Sentimento com Deep Learning

---

## O que s√£o Transformers?

- Arquitetura de **Deep Learning** para Processamento de Linguagem Natural (NLP)
- Baseada em **Attention Mechanism** (Mecanismo de Aten√ß√£o)
- Revolucionaram o campo de NLP a partir de 2017
- Modelos famosos: **BERT**, **GPT**, **RoBERTa**

---

## Por que "Transformers"?

- **Transformam** sequ√™ncias de texto em representa√ß√µes num√©ricas
- Permitem que o modelo **preste aten√ß√£o** em todas as palavras simultaneamente
- Diferente de m√©todos antigos que processam palavra por palavra

---

## Attention Mechanism (Mecanismo de Aten√ß√£o)

### Como funciona?

- O modelo "presta aten√ß√£o" em **todas as palavras** ao mesmo tempo
- Entende **rela√ß√µes** entre palavras distantes
- Captura **contexto profundo** do texto

### Analogia:
> √â como ler um texto inteiro de uma vez, em vez de palavra por palavra

---

## Compara√ß√£o: M√©todos Tradicionais vs Transformers

### M√©todos Tradicionais (ex: TextBlob)
- ‚ùå Baseados em **dicion√°rios** de palavras
- ‚ùå N√£o entendem **contexto profundo**
- ‚ùå Dificuldade com **sarcasmo** e **ironia**
- ‚ùå Processam palavras **isoladamente**

### Transformers
- ‚úÖ Entendem **contexto completo**
- ‚úÖ Treinados em **milh√µes de exemplos**
- ‚úÖ Capturam **nuances** lingu√≠sticas
- ‚úÖ Processam **todo o texto** simultaneamente

---

## Exemplo Pr√°tico: An√°lise de Sentimento

### Tweet: 
> "Great! My flight was cancelled and I'm stuck at the airport."

### TextBlob (Tradicional):
- V√™ "Great!" ‚Üí **Positivo** ‚ùå
- Ignora o contexto negativo

### Transformers:
- Analisa todo o texto ‚Üí **Negativo** ‚úÖ
- Entende o sarcasmo

---

## Vantagens dos Transformers

1. **Contexto Profundo**
   - Entendem significado completo, n√£o apenas palavras isoladas

2. **Aprendizado**
   - Treinados em milh√µes de exemplos de dados reais

3. **Nuances**
   - Capturam sarcasmo, ironia e contexto cultural

4. **Confian√ßa**
   - Fornecem scores de confian√ßa para cada predi√ß√£o

---

## Arquitetura dos Transformers

### Componentes principais:

1. **Encoder** - Processa o texto de entrada
2. **Attention Layers** - Calcula aten√ß√£o entre palavras
3. **Feed-Forward Networks** - Processa as representa√ß√µes
4. **Output Layer** - Gera a predi√ß√£o final

---

## Modelos Pr√©-treinados

### Vantagem:
- Modelos j√° **treinados** em grandes volumes de dados
- Podem ser **reutilizados** para diferentes tarefas
- **Fine-tuning** para tarefas espec√≠ficas

### Exemplos:
- BERT (Google)
- GPT (OpenAI)
- RoBERTa (Facebook)
- T5 (Google)

---

## Aplica√ß√µes dos Transformers

- üì± **An√°lise de Sentimento** em redes sociais
- üìù **Tradu√ß√£o Autom√°tica**
- üí¨ **Chatbots** e assistentes virtuais
- üìä **An√°lise de Reviews** de produtos
- üîç **Busca Sem√¢ntica**
- üì∞ **Resumo Autom√°tico** de textos

---

## Como Usar Transformers?

### Biblioteca Hugging Face

```python
from transformers import pipeline

# Carregar modelo pr√©-treinado
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="cardiffnlp/twitter-roberta-base-sentiment-latest"
)

# Usar o modelo
resultado = sentiment_analyzer("I love this product!")
```

---

## Resultados: TextBlob vs Transformers

### Estat√≠sticas dos Testes:

- **TextBlob acertou:** 33.3% dos casos
- **Transformers acertou:** 41.7% dos casos

### Casos de Discord√¢ncia:

- Transformers s√£o **superiores** em:
  - Sarcasmo e ironia
  - Contexto negativo com palavras positivas
  - Constru√ß√µes lingu√≠sticas complexas

---

## Exemplo Real: Sarcasmo

### Tweet:
> "This is the worst service I've ever experienced"

### TextBlob:
- **Neutro** (polaridade -0.1) ‚ùå
- N√£o detecta sentimento negativo forte

### Transformers:
- **Negativo** (confian√ßa 0.95) ‚úÖ
- Entende corretamente o sentimento

---

## Por que Transformers s√£o Melhores?

### 1. Entendimento de Contexto
- N√£o apenas palavras, mas **rela√ß√µes** entre elas

### 2. Aprendizado Profundo
- Treinados em **milh√µes** de exemplos

### 3. Especializa√ß√£o
- Podem ser treinados para **dom√≠nios espec√≠ficos**
- Ex: An√°lise de tweets, reviews, etc.

---

## Limita√ß√µes dos Transformers

- ‚ö†Ô∏è Requerem **muito poder computacional** para treinar
- ‚ö†Ô∏è Modelos grandes podem ser **lentos** para infer√™ncia
- ‚ö†Ô∏è Sarcasmo muito sutil ainda √© **desafiador**
- ‚ö†Ô∏è Dependem da **qualidade dos dados** de treinamento

---

## Impacto dos Transformers

### Revolu√ß√£o no NLP:

- **2017:** Paper "Attention Is All You Need"
- **2018:** BERT lan√ßado pelo Google
- **2019:** GPT-2 demonstra capacidades impressionantes
- **2020+:** GPT-3, GPT-4, e outros modelos grandes

### Hoje:
- Transformers s√£o **essenciais** para NLP moderno
- Usados em produtos do dia a dia (Google, ChatGPT, etc.)

---

## Conclus√£o

### Transformers s√£o:

‚úÖ **Superiores** a m√©todos tradicionais  
‚úÖ **Entendem contexto** profundo  
‚úÖ **Capturam nuances** lingu√≠sticas  
‚úÖ **F√°ceis de usar** com bibliotecas modernas  

### Para an√°lise de sentimento:
- Transformers s√£o a **melhor escolha** atual
- Especialmente para dados de redes sociais
- Melhor precis√£o e entendimento de contexto

---

## Obrigado!

### Perguntas?

---

## Refer√™ncias

- Paper original: "Attention Is All You Need" (Vaswani et al., 2017)
- Hugging Face: https://huggingface.co/
- Modelo usado: `cardiffnlp/twitter-roberta-base-sentiment-latest`


