---
marp: true
theme: default
paginate: true
---

# ğŸ¤– Transformers
## Como as mÃ¡quinas aprendem a entender linguagem

Uma explicaÃ§Ã£o simples para leigos

---

## O Problema

**Como fazer uma mÃ¡quina entender texto?**

- Computadores sÃ³ entendem nÃºmeros (0 e 1)
- Mas nÃ³s falamos em palavras
- Como converter palavras em nÃºmeros que faÃ§am sentido?

> **Analogia:** Ã‰ como traduzir portuguÃªs para uma linguagem que sÃ³ tem nÃºmeros!

---

## De Onde Vieram os Transformers?

### ComeÃ§ou com Redes Neurais

**1940s:** Primeiras ideias - NeurÃ´nios artificiais (inspirados no cÃ©rebro humano)

**1980s:** Redes Neurais - MÃºltiplas camadas de neurÃ´nios conectados

**2010s:** Deep Learning - Redes muito profundas com muitas camadas

---

## EvoluÃ§Ã£o no Processamento de Texto

**Anos 2000:** MÃ©todos EstatÃ­sticos - Contagem de palavras, regras simples

**2010-2015:** Word Embeddings - Palavras viram vetores (nÃºmeros)
- Ex: "rei" - "homem" + "mulher" = "rainha"

**2015-2017:** RNNs/LSTMs - Processavam texto palavra por palavra (sequencial)

---

## 2017: O Nascimento dos Transformers

### Paper: "Attention Is All You Need"
Google publicou uma nova arquitetura revolucionÃ¡ria

- **Problema anterior:** Processar palavra por palavra era lento
- **SoluÃ§Ã£o:** Processar TODAS as palavras ao mesmo tempo!
- **InovaÃ§Ã£o:** Mecanismo de AtenÃ§Ã£o

> **Antes:** Ler um livro palavra por palavra  
> **Agora:** Ver o livro inteiro de uma vez e entender tudo!

---

## Word Embedding: Convertendo Palavras em NÃºmeros

### Como funciona?

Cada palavra vira um **vetor** (lista de nÃºmeros)

```
"gato" â†’ [0.2, -0.5, 0.8, 0.1, ...]
"cachorro" â†’ [0.3, -0.4, 0.7, 0.2, ...]
"animal" â†’ [0.25, -0.45, 0.75, 0.15, ...]
```

> **Analogia:** Ã‰ como criar um "mapa" onde palavras similares ficam prÃ³ximas.  
> "Gato" e "cachorro" tÃªm nÃºmeros parecidos porque sÃ£o ambos animais!

---

## Mecanismo de AtenÃ§Ã£o: O CoraÃ§Ã£o dos Transformers

### O que Ã©?

O modelo "presta atenÃ§Ã£o" em todas as palavras ao mesmo tempo

**Exemplo:**
- **Frase:** "O banco estÃ¡ fechado"
- Qual "banco"? Banco de sentar ou banco financeiro?
- AtenÃ§Ã£o olha para "fechado" â†’ Ah, Ã© banco financeiro!

> **Analogia:** Ã‰ como quando vocÃª lÃª uma frase e seu cÃ©rebro automaticamente conecta todas as palavras para entender o significado completo.

---

## Como o Mecanismo de AtenÃ§Ã£o Funciona?

### Passo a Passo:

1. Cada palavra olha para **TODAS** as outras palavras
2. Calcula "quanto" cada palavra Ã© importante para entender as outras
3. Cria uma "rede de conexÃµes" entre todas as palavras
4. Usa essas conexÃµes para entender o contexto completo

> **Visual:** Imagine uma teia de aranha conectando todas as palavras!

---

## Arquitetura: Encoder e Decoder

### Encoder (Codificador)
- ğŸ“¥ Recebe o texto de entrada
- ğŸ” Processa e entende o texto
- ğŸ’¾ Cria uma representaÃ§Ã£o interna

### Decoder (Decodificador)
- ğŸ“¤ Pega a representaÃ§Ã£o do encoder
- ğŸ¯ Gera a saÃ­da (traduÃ§Ã£o, resposta, etc.)
- âœ¨ Produz o resultado final

> **Analogia:** Encoder = tradutor que entende portuguÃªs  
> Decoder = tradutor que fala inglÃªs e produz a traduÃ§Ã£o

---

## Fluxo Completo: Do Texto ao Resultado

1. **Word Embedding**
   - "OlÃ¡ mundo" â†’ [nÃºmeros, nÃºmeros, ...]

2. **Encoder**
   - Processa com Attention â†’ Entende contexto

3. **Decoder**
   - Gera resposta â†’ "Hello world"

> **Tudo isso acontece simultaneamente!**  
> NÃ£o precisa processar palavra por palavra

---

## Por que Transformers sÃ£o Melhores?

- âš¡ **Mais RÃ¡pido:** Processa tudo ao mesmo tempo
- ğŸ§  **Entende Contexto:** VÃª o texto completo
- ğŸ“š **Aprende Melhor:** Treinado em milhÃµes de textos
- ğŸ”„ **ReutilizÃ¡vel:** Um modelo serve para vÃ¡rias tarefas

> **ComparaÃ§Ã£o:**  
> **Antes:** Como ler um livro pÃ¡gina por pÃ¡gina  
> **Agora:** Como ter uma visÃ£o aÃ©rea de todo o livro de uma vez!

---

## Onde VocÃª JÃ¡ Viu Transformers?

- ğŸ’¬ **ChatGPT:** Conversa com vocÃª
- ğŸŒ **Google Translate:** Traduz textos
- ğŸ” **Busca Google:** Entende o que vocÃª procura
- ğŸ“± **Assistentes Virtuais:** Siri, Alexa, etc.
- ğŸ“Š **AnÃ¡lise de Sentimento:** Entende se texto Ã© positivo/negativo

> **Transformers estÃ£o em todo lugar!**  
> VocÃª provavelmente jÃ¡ usou sem saber!

---

## Resumo: Como Funciona?

**Texto de Entrada**
- "O gato estÃ¡ no tapete"

â†“

**Word Embedding**
- Palavras â†’ Vetores (nÃºmeros)

â†“

**Encoder + Attention**
- Entende contexto e relaÃ§Ãµes

â†“

**Decoder**
- Gera resultado (traduÃ§Ã£o, anÃ¡lise, etc.)

> **Em uma frase:** Transformers convertem palavras em nÃºmeros, entendem o contexto completo usando atenÃ§Ã£o, e produzem resultados inteligentes!

---

## ConclusÃ£o

### Transformers sÃ£o:

- Uma evoluÃ§Ã£o das redes neurais
- Usam mecanismo de atenÃ§Ã£o para entender contexto
- Convertem palavras em nÃºmeros (word embeddings)
- TÃªm encoder (entende) e decoder (gera resposta)
- Revolucionaram como mÃ¡quinas processam linguagem

> **Em resumo:** Transformers sÃ£o como dar "superpoderes" de compreensÃ£o de linguagem para computadores!

---

## Obrigado!

### Perguntas?


