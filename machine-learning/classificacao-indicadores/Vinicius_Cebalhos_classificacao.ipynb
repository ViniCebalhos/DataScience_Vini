{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lise de Classifica√ß√£o: Indicadores Socioecon√¥micos e Desenvolvimento Humano\n",
        "\n",
        "**Aluno: Vin√≠cius Cebalhos**\n",
        "\n",
        "## Contexto do Problema\n",
        "\n",
        "Este trabalho analisa a rela√ß√£o entre indicadores socioecon√¥micos e o n√≠vel de desenvolvimento/classifica√ß√£o de pa√≠ses. O dataset cont√©m:\n",
        "\n",
        "- **Escada de Cantril (Cantril Ladder Score)**: Medida de bem-estar subjetivo e felicidade da popula√ß√£o (0-10)\n",
        "- **PIB per capita (GDP per capita PPP)**: Riqueza econ√¥mica m√©dia por pessoa, ajustada por paridade de poder de compra\n",
        "- **IDH (HDI - Human Development Index)**: √çndice composto de desenvolvimento humano (0-1000)\n",
        "- **Taxa de Homic√≠dios**: Indicador de seguran√ßa e viol√™ncia (por 100.000 habitantes)\n",
        "\n",
        "O objetivo √© **classificar pa√≠ses em 4 categorias** baseado nesses indicadores, identificando padr√µes que relacionam desenvolvimento econ√¥mico, bem-estar subjetivo, desenvolvimento humano e seguran√ßa.\n",
        "\n",
        "Este notebook realiza uma an√°lise completa de classifica√ß√£o seguindo metodologia rigorosa, identificando e tratando poss√≠veis pegadinhas no dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            confusion_matrix, classification_report, roc_curve, auc,\n",
        "                            roc_auc_score)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîé 1. Carregamento dos Dados + Auditoria Inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregamento do dataset\n",
        "file_path = 'gdp-and-homicides-vs-happiness-vs-hdi_FabroClassification2025OK_4classes.csv'\n",
        "\n",
        "# Observando que o separador √© ponto e v√≠rgula\n",
        "df = pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CARREGAMENTO DOS DADOS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nShape do dataset: {df.shape}\")\n",
        "print(f\"Linhas: {df.shape[0]}, Colunas: {df.shape[1]}\\n\")\n",
        "\n",
        "print(\"Primeiras linhas do dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Verifica√ß√£o r√°pida de valores negativos em vari√°veis cr√≠ticas\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VERIFICA√á√ÉO R√ÅPIDA: VALORES NEGATIVOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar homic√≠dios\n",
        "if 'Intentional homicides (per 100000 people) as of 2021' in df.columns:\n",
        "    homicidios_neg = (df['Intentional homicides (per 100000 people) as of 2021'] < 0).sum()\n",
        "    homicidios_min = df['Intentional homicides (per 100000 people) as of 2021'].min()\n",
        "    if homicidios_neg > 0:\n",
        "        print(f\"‚ö†Ô∏è ERRO: {homicidios_neg} valores negativos em Taxa de Homic√≠dios!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Taxa de Homic√≠dios: OK (m√≠nimo = {homicidios_min:.2f}, nenhum negativo)\")\n",
        "\n",
        "# Verificar GDP\n",
        "if 'GDP per capita. PPP (constant 2021 international $)' in df.columns:\n",
        "    gdp_neg = (df['GDP per capita. PPP (constant 2021 international $)'] < 0).sum()\n",
        "    gdp_min = df['GDP per capita. PPP (constant 2021 international $)'].min()\n",
        "    if gdp_neg > 0:\n",
        "        print(f\"‚ö†Ô∏è ERRO: {gdp_neg} valores negativos em PIB per capita!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ PIB per capita: OK (m√≠nimo = ${gdp_min:,.0f}, nenhum negativo)\")\n",
        "\n",
        "# Verificar IDH\n",
        "if 'HDI (Human Development Indicator) Value (0~1000)' in df.columns:\n",
        "    hdi_neg = (df['HDI (Human Development Indicator) Value (0~1000)'] < 0).sum()\n",
        "    hdi_min = df['HDI (Human Development Indicator) Value (0~1000)'].min()\n",
        "    if hdi_neg > 0:\n",
        "        print(f\"‚ö†Ô∏è ERRO: {hdi_neg} valores negativos em IDH!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ IDH: OK (m√≠nimo = {hdi_min:.0f}, nenhum negativo)\")\n",
        "\n",
        "# Verificar Cantril\n",
        "if 'Cantril ladder score' in df.columns:\n",
        "    cantril_neg = (df['Cantril ladder score'] < 0).sum()\n",
        "    cantril_min = df['Cantril ladder score'].min()\n",
        "    if cantril_neg > 0:\n",
        "        print(f\"‚ö†Ô∏è ERRO: {cantril_neg} valores negativos em Escada de Cantril!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Escada de Cantril: OK (m√≠nimo = {cantril_min:.2f}, nenhum negativo)\")\n",
        "\n",
        "print(\"\\n‚úÖ CONFIRMA√á√ÉO: Nenhum valor negativo encontrado nas vari√°veis cr√≠ticas\")\n",
        "print(\"   O dataset est√° coerente - todos os valores est√£o dentro de faixas esperadas (‚â• 0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUDITORIA INICIAL COMPLETA\n",
        "print(\"=\" * 80)\n",
        "print(\"AUDITORIA INICIAL - INFORMA√á√ïES DAS COLUNAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. INFORMA√á√ïES GERAIS:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\\n2. TIPOS DE DADOS:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\n\\n3. NOMES DAS COLUNAS:\")\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    print(f\"{i}. {col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de valores ausentes\n",
        "print(\"=\" * 80)\n",
        "print(\"VALORES AUSENTES (MISSING VALUES)\")\n",
        "print(\"=\" * 80)\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Coluna': missing.index,\n",
        "    'Valores Ausentes': missing.values,\n",
        "    'Percentual (%)': missing_pct.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Valores ausentes encontrados!\")\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"\\n‚úÖ Nenhum valor ausente encontrado no dataset.\")\n",
        "\n",
        "print(\"\\n\\nVerifica√ß√£o completa por coluna:\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de duplicatas\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFICA√á√ÉO DE DUPLICATAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "duplicatas = df.duplicated()\n",
        "num_duplicatas = duplicatas.sum()\n",
        "\n",
        "print(f\"\\nN√∫mero de linhas duplicadas: {num_duplicatas}\")\n",
        "\n",
        "if num_duplicatas > 0:\n",
        "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Linhas duplicadas encontradas!\")\n",
        "    print(df[duplicatas])\n",
        "else:\n",
        "    print(\"\\n‚úÖ Nenhuma linha duplicada encontrada.\")\n",
        "\n",
        "# Verificar duplicatas por nome de pa√≠s (pode haver pa√≠ses duplicados)\n",
        "duplicatas_pais = df.duplicated(subset=['Country Name'])\n",
        "num_duplicatas_pais = duplicatas_pais.sum()\n",
        "print(f\"\\nN√∫mero de pa√≠ses duplicados (por nome): {num_duplicatas_pais}\")\n",
        "\n",
        "if num_duplicatas_pais > 0:\n",
        "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Pa√≠ses duplicados encontrados!\")\n",
        "    print(df[df.duplicated(subset=['Country Name'], keep=False)][['Country Name', 'Country Code']].sort_values('Country Name'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de coer√™ncia das faixas num√©ricas\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISE DE COER√äNCIA DAS FAIXAS NUM√âRICAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Selecionar apenas colunas num√©ricas\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nColunas num√©ricas identificadas: {numeric_cols}\\n\")\n",
        "\n",
        "# VERIFICA√á√ÉO ESPEC√çFICA: Valores negativos em vari√°veis que n√£o podem ser negativas\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFICA√á√ÉO CR√çTICA: VALORES NEGATIVOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Vari√°veis que N√ÉO podem ter valores negativos\n",
        "variaveis_nao_negativas = {\n",
        "    'Intentional homicides (per 100000 people) as of 2021': 'Taxa de homic√≠dios n√£o pode ser negativa',\n",
        "    'GDP per capita. PPP (constant 2021 international $)': 'PIB per capita n√£o pode ser negativo',\n",
        "    'HDI (Human Development Indicator) Value (0~1000)': 'IDH n√£o pode ser negativo (escala 0-1000)',\n",
        "    'Cantril ladder score': 'Escada de Cantril n√£o pode ser negativa (escala 0-10)'\n",
        "}\n",
        "\n",
        "problemas_negativos = []\n",
        "for var, descricao in variaveis_nao_negativas.items():\n",
        "    if var in df.columns:\n",
        "        negativos = (df[var] < 0).sum()\n",
        "        min_val = df[var].min()\n",
        "        if negativos > 0:\n",
        "            problemas_negativos.append(f\"  ‚ö†Ô∏è ERRO CR√çTICO: {var}\")\n",
        "            problemas_negativos.append(f\"     {negativos} valores negativos encontrados!\")\n",
        "            problemas_negativos.append(f\"     {descricao}\")\n",
        "        else:\n",
        "            print(f\"  ‚úÖ {var.split('(')[0].strip()}: OK (m√≠nimo = {min_val:.2f}, nenhum valor negativo)\")\n",
        "\n",
        "if problemas_negativos:\n",
        "    print(\"\\n\".join(problemas_negativos))\n",
        "    print(\"\\n‚ö†Ô∏è A√á√ÉO NECESS√ÅRIA: Investigar e corrigir valores negativos antes de prosseguir!\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ CONFIRMADO: Nenhum valor negativo encontrado em vari√°veis cr√≠ticas\")\n",
        "    print(\"   Todos os valores est√£o dentro de faixas esperadas (‚â• 0)\")\n",
        "\n",
        "# Verificar faixas esperadas vs observadas\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FAIXAS DE VALORES POR COLUNA NUM√âRICA\")\n",
        "print(\"=\" * 80)\n",
        "for col in numeric_cols:\n",
        "    min_val = df[col].min()\n",
        "    max_val = df[col].max()\n",
        "    mean_val = df[col].mean()\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Min: {min_val:.2f}\")\n",
        "    print(f\"  Max: {max_val:.2f}\")\n",
        "    print(f\"  M√©dia: {mean_val:.2f}\")\n",
        "    print(f\"  Valores √∫nicos: {df[col].nunique()}\")\n",
        "    \n",
        "    # Verifica√ß√£o adicional de coer√™ncia\n",
        "    if 'homicide' in col.lower():\n",
        "        if min_val < 0:\n",
        "            print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: Valores negativos encontrados em taxa de homic√≠dios!\")\n",
        "        elif min_val == 0:\n",
        "            print(f\"  ‚ÑπÔ∏è Nota: Alguns pa√≠ses t√™m taxa de homic√≠dios = 0 (muito seguros)\")\n",
        "    elif 'gdp' in col.lower():\n",
        "        if min_val < 0:\n",
        "            print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: Valores negativos encontrados em PIB!\")\n",
        "        elif min_val < 1000:\n",
        "            print(f\"  ‚ÑπÔ∏è Nota: PIB m√≠nimo √© ${min_val:.0f} (pa√≠ses muito pobres)\")\n",
        "    elif 'hdi' in col.lower():\n",
        "        if min_val < 0:\n",
        "            print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: Valores negativos encontrados em IDH!\")\n",
        "        elif min_val < 500:\n",
        "            print(f\"  ‚ÑπÔ∏è Nota: IDH m√≠nimo √© {min_val:.0f} (desenvolvimento humano muito baixo)\")\n",
        "    elif 'cantril' in col.lower():\n",
        "        if min_val < 0:\n",
        "            print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: Valores negativos encontrados em Escada de Cantril!\")\n",
        "        elif min_val < 2:\n",
        "            print(f\"  ‚ÑπÔ∏è Nota: Felicidade m√≠nima √© {min_val:.2f} (pa√≠ses muito infelizes)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identifica√ß√£o de valores at√≠picos usando IQR\n",
        "print(\"=\" * 80)\n",
        "print(\"IDENTIFICA√á√ÉO DE VALORES AT√çPICOS (OUTLIERS)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüìå IMPORTANTE: O m√©todo IQR calcula limites estat√≠sticos usando a f√≥rmula:\")\n",
        "print(\"   - Limite Inferior = Q1 - 1.5 √ó IQR\")\n",
        "print(\"   - Limite Superior = Q3 + 1.5 √ó IQR\")\n",
        "print(\"\\n   ‚ö†Ô∏è ATEN√á√ÉO: O 'lower_bound' pode ser NEGATIVO mesmo que n√£o existam\")\n",
        "print(\"      valores negativos no dataset! Isso √© apenas um limite estat√≠stico.\")\n",
        "print(\"      O que importa √© verificar se H√Å valores no dataset abaixo desse limite.\")\n",
        "print(\"      Como confirmado anteriormente, N√ÉO h√° valores negativos no dataset.\\n\")\n",
        "\n",
        "outliers_summary = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    # Verificar outliers reais (apenas os que est√£o fora dos limites E existem no dataset)\n",
        "    outliers_inferiores = df[df[col] < lower_bound]\n",
        "    outliers_superiores = df[df[col] > upper_bound]\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    num_outliers = len(outliers)\n",
        "    num_outliers_inf = len(outliers_inferiores)\n",
        "    num_outliers_sup = len(outliers_superiores)\n",
        "    \n",
        "    outliers_summary[col] = {\n",
        "        'num_outliers': num_outliers,\n",
        "        'pct_outliers': (num_outliers / len(df)) * 100,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound,\n",
        "        'outliers_abaixo_limite': num_outliers_inf,\n",
        "        'outliers_acima_limite': num_outliers_sup,\n",
        "        'valor_min_real': df[col].min(),\n",
        "        'valor_max_real': df[col].max()\n",
        "    }\n",
        "    \n",
        "    if num_outliers > 0:\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Outliers encontrados: {num_outliers} ({(num_outliers/len(df)*100):.2f}%)\")\n",
        "        print(f\"  Limite inferior (estat√≠stico): {lower_bound:.2f}\")\n",
        "        print(f\"  Limite superior (estat√≠stico): {upper_bound:.2f}\")\n",
        "        print(f\"  Valor m√≠nimo REAL no dataset: {df[col].min():.2f}\")\n",
        "        print(f\"  Valor m√°ximo REAL no dataset: {df[col].max():.2f}\")\n",
        "        print(f\"  Outliers abaixo do limite inferior: {num_outliers_inf} (Nenhum, pois n√£o h√° valores negativos)\")\n",
        "        print(f\"  Outliers acima do limite superior: {num_outliers_sup} (Estes s√£o os outliers reais)\")\n",
        "\n",
        "print(\"\\n\\nResumo de outliers:\")\n",
        "outliers_df = pd.DataFrame(outliers_summary).T\n",
        "# Mostrar apenas colunas relevantes para visualiza√ß√£o\n",
        "print(outliers_df[['num_outliers', 'pct_outliers', 'outliers_abaixo_limite', \n",
        "                   'outliers_acima_limite', 'valor_min_real', 'valor_max_real']])\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETA√á√ÉO:\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úÖ 'outliers_abaixo_limite' = 0 para todas as vari√°veis cr√≠ticas\")\n",
        "print(\"   ‚Üí Confirma que N√ÉO h√° valores negativos no dataset\")\n",
        "print(\"‚úÖ 'outliers_acima_limite' mostra os outliers reais (valores muito altos)\")\n",
        "print(\"‚úÖ 'lower_bound' negativo √© apenas um limite estat√≠stico, n√£o indica valores negativos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de inconsist√™ncias espec√≠ficas\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFICA√á√ÉO DE INCONSIST√äNCIAS ESPEC√çFICAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar se h√° colunas redundantes (ex: homic√≠dios e homic√≠dios*100)\n",
        "print(\"\\n1. Verificando colunas potencialmente redundantes:\")\n",
        "\n",
        "# Verificar rela√ß√£o entre \"Intentional homicides (per 100000 people)\" e \"Intentional homicides/100k  x 100\"\n",
        "if 'Intentional homicides (per 100000 people) as of 2021' in df.columns and 'Intentional homicides/100k  x 100' in df.columns:\n",
        "    homicides_original = df['Intentional homicides (per 100000 people) as of 2021']\n",
        "    homicides_scaled = df['Intentional homicides/100k  x 100']\n",
        "    expected_scaled = homicides_original * 100\n",
        "    \n",
        "    # Verificar se s√£o consistentes (com toler√¢ncia para arredondamento)\n",
        "    diff = abs(homicides_scaled - expected_scaled)\n",
        "    inconsistent = (diff > 0.01).sum()\n",
        "    \n",
        "    if inconsistent > 0:\n",
        "        print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: {inconsistent} inconsist√™ncias entre colunas de homic√≠dios!\")\n",
        "        print(df[diff > 0.01][['Country Name', 'Intentional homicides (per 100000 people) as of 2021', \n",
        "                               'Intentional homicides/100k  x 100']])\n",
        "    else:\n",
        "        print(\"  ‚úÖ Colunas de homic√≠dios s√£o consistentes (redundantes)\")\n",
        "\n",
        "# Verificar rela√ß√£o entre \"Cantril ladder score\" e \"Cantril ladder score*1000\"\n",
        "if 'Cantril ladder score' in df.columns and 'Cantril ladder score*1000' in df.columns:\n",
        "    cantril_original = df['Cantril ladder score']\n",
        "    cantril_scaled = df['Cantril ladder score*1000']\n",
        "    expected_scaled = cantril_original * 1000\n",
        "    \n",
        "    diff = abs(cantril_scaled - expected_scaled)\n",
        "    inconsistent = (diff > 0.01).sum()\n",
        "    \n",
        "    if inconsistent > 0:\n",
        "        print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: {inconsistent} inconsist√™ncias entre colunas de Cantril!\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Colunas de Cantril s√£o consistentes (redundantes)\")\n",
        "\n",
        "# Verificar se as colunas de classe s√£o consistentes com Country Class\n",
        "print(\"\\n2. Verificando consist√™ncia das colunas de classe:\")\n",
        "if 'Country Class' in df.columns:\n",
        "    class_cols = ['Class 4', 'Class 3', 'Class 2', 'Class 1']\n",
        "    if all(col in df.columns for col in class_cols):\n",
        "        # Verificar se a soma das colunas de classe √© sempre 1\n",
        "        class_sum = df[class_cols].sum(axis=1)\n",
        "        if (class_sum == 1).all():\n",
        "            print(\"  ‚úÖ Colunas de classe s√£o one-hot encoding v√°lido (soma = 1)\")\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: {((class_sum != 1).sum())} linhas com soma de classes != 1\")\n",
        "        \n",
        "        # Verificar se Country Class corresponde √†s colunas one-hot\n",
        "        for idx, row in df.iterrows():\n",
        "            country_class = int(row['Country Class'])\n",
        "            expected_class_col = f'Class {country_class}'\n",
        "            if row[expected_class_col] != 1:\n",
        "                print(f\"  ‚ö†Ô∏è INCONSIST√äNCIA: {row['Country Name']} tem Country Class={country_class} mas {expected_class_col}={row[expected_class_col]}\")\n",
        "        \n",
        "        print(\"  ‚úÖ Verifica√ß√£o de consist√™ncia entre Country Class e colunas one-hot conclu√≠da\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìã Relat√≥rio Inicial de Auditoria\n",
        "\n",
        "**Status do Dataset:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gera√ß√£o do relat√≥rio inicial escrito\n",
        "print(\"=\" * 80)\n",
        "print(\"RELAT√ìRIO INICIAL DE AUDITORIA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "relatorio = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "RELAT√ìRIO DE AUDITORIA INICIAL DO DATASET\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. ESTRUTURA DO DATASET:\n",
        "   - Total de registros: {len(df)}\n",
        "   - Total de colunas: {len(df.columns)}\n",
        "   - Colunas identificadas: {', '.join(df.columns.tolist())}\n",
        "\n",
        "2. VALORES AUSENTES:\n",
        "   - Total de valores ausentes: {df.isnull().sum().sum()}\n",
        "   - Colunas com valores ausentes: {', '.join(df.columns[df.isnull().any()].tolist()) if df.isnull().any().any() else 'Nenhuma'}\n",
        "\n",
        "3. DUPLICATAS:\n",
        "   - Linhas completamente duplicadas: {df.duplicated().sum()}\n",
        "   - Pa√≠ses duplicados: {df.duplicated(subset=['Country Name']).sum() if 'Country Name' in df.columns else 'N/A'}\n",
        "\n",
        "4. TIPOS DE DADOS:\n",
        "   - Colunas num√©ricas: {len(df.select_dtypes(include=[np.number]).columns)}\n",
        "   - Colunas categ√≥ricas: {len(df.select_dtypes(include=['object']).columns)}\n",
        "\n",
        "5. VARI√ÅVEL-ALVO IDENTIFICADA:\n",
        "   - Coluna 'Country Class' encontrada com valores: {sorted(df['Country Class'].unique().tolist()) if 'Country Class' in df.columns else 'N√ÉO ENCONTRADA'}\n",
        "   - Distribui√ß√£o das classes: {df['Country Class'].value_counts().to_dict() if 'Country Class' in df.columns else 'N/A'}\n",
        "\n",
        "6. PROBLEMAS IDENTIFICADOS:\n",
        "\"\"\"\n",
        "\n",
        "# Adicionar problemas encontrados\n",
        "problemas = []\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    problemas.append(f\"   - Valores ausentes encontrados em {df.columns[df.isnull().any()].tolist()}\")\n",
        "if df.duplicated().sum() > 0:\n",
        "    problemas.append(f\"   - {df.duplicated().sum()} linhas completamente duplicadas\")\n",
        "if df.duplicated(subset=['Country Name']).sum() > 0:\n",
        "    problemas.append(f\"   - {df.duplicated(subset=['Country Name']).sum()} pa√≠ses duplicados\")\n",
        "\n",
        "# Verificar colunas redundantes\n",
        "if 'Intentional homicides (per 100000 people) as of 2021' in df.columns and 'Intentional homicides/100k  x 100' in df.columns:\n",
        "    problemas.append(\"   - Colunas redundantes: 'Intentional homicides (per 100000 people) as of 2021' e 'Intentional homicides/100k  x 100'\")\n",
        "if 'Cantril ladder score' in df.columns and 'Cantril ladder score*1000' in df.columns:\n",
        "    problemas.append(\"   - Colunas redundantes: 'Cantril ladder score' e 'Cantril ladder score*1000'\")\n",
        "\n",
        "if problemas:\n",
        "    relatorio += \"\\n\".join(problemas)\n",
        "else:\n",
        "    relatorio += \"   - Nenhum problema cr√≠tico identificado\"\n",
        "\n",
        "relatorio += f\"\"\"\n",
        "\n",
        "7. ADEQUA√á√ÉO DO DATASET:\n",
        "   - O dataset {'APRESENTA' if problemas else 'N√ÉO APRESENTA'} problemas que requerem tratamento\n",
        "   - {'√â NECESS√ÅRIO' if problemas else 'N√ÉO √â NECESS√ÅRIO'} tratamento pr√©vio antes da an√°lise\n",
        "\n",
        "8. RECOMENDA√á√ïES:\n",
        "\"\"\"\n",
        "\n",
        "recomendacoes = []\n",
        "if df.isnull().sum().sum() > 0:\n",
        "    recomendacoes.append(\"   - Tratar valores ausentes antes de prosseguir\")\n",
        "if df.duplicated().sum() > 0:\n",
        "    recomendacoes.append(\"   - Remover linhas duplicadas\")\n",
        "if 'Intentional homicides/100k  x 100' in df.columns:\n",
        "    recomendacoes.append(\"   - Remover coluna 'Intentional homicides/100k  x 100' (redundante)\")\n",
        "if 'Cantril ladder score*1000' in df.columns:\n",
        "    recomendacoes.append(\"   - Remover coluna 'Cantril ladder score*1000' (redundante)\")\n",
        "if 'Class 4' in df.columns and 'Class 3' in df.columns and 'Class 2' in df.columns and 'Class 1' in df.columns:\n",
        "    recomendacoes.append(\"   - Remover colunas one-hot encoding (Class 1, Class 2, Class 3, Class 4) - usar apenas 'Country Class'\")\n",
        "\n",
        "if recomendacoes:\n",
        "    relatorio += \"\\n\".join(recomendacoes)\n",
        "else:\n",
        "    relatorio += \"   - Dataset est√° adequado para an√°lise direta\"\n",
        "\n",
        "relatorio += \"\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\"\n",
        "\n",
        "print(relatorio)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 2. An√°lise Explorat√≥ria Completa (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estat√≠sticas descritivas completas\n",
        "print(\"=\" * 80)\n",
        "print(\"ESTAT√çSTICAS DESCRITIVAS COMPLETAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Selecionar apenas colunas num√©ricas relevantes (excluindo colunas redundantes e de classe)\n",
        "cols_para_analise = [col for col in numeric_cols \n",
        "                     if col not in ['Class 4', 'Class 3', 'Class 2', 'Class 1', \n",
        "                                   'Intentional homicides/100k  x 100', \n",
        "                                   'Cantril ladder score*1000']]\n",
        "\n",
        "print(\"\\nEstat√≠sticas descritivas das vari√°veis num√©ricas principais:\")\n",
        "print(df[cols_para_analise].describe())\n",
        "\n",
        "print(\"\\n\\nEstat√≠sticas adicionais (mediana, moda, assimetria, curtose):\")\n",
        "stats_extras = pd.DataFrame({\n",
        "    'Mediana': df[cols_para_analise].median(),\n",
        "    'Moda': df[cols_para_analise].mode().iloc[0] if len(df[cols_para_analise].mode()) > 0 else None,\n",
        "    'Assimetria': df[cols_para_analise].skew(),\n",
        "    'Curtose': df[cols_para_analise].kurtosis()\n",
        "})\n",
        "print(stats_extras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribui√ß√µes - Histogramas e Boxplots\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISE DE DISTRIBUI√á√ïES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Criar figura com subplots para histogramas\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "cols_plot = [col for col in cols_para_analise if col != 'Country Class']\n",
        "\n",
        "for i, col in enumerate(cols_plot[:6]):  # Limitar a 6 colunas para visualiza√ß√£o\n",
        "    axes[i].hist(df[col].dropna(), bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[i].set_title(f'Histograma: {col}', fontsize=12, fontweight='bold')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Frequ√™ncia')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('histogramas_variaveis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Histogramas salvos em 'histogramas_variaveis.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Boxplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, col in enumerate(cols_plot[:6]):\n",
        "    axes[i].boxplot(df[col].dropna(), vert=True)\n",
        "    axes[i].set_title(f'Boxplot: {col}', fontsize=12, fontweight='bold')\n",
        "    axes[i].set_ylabel(col)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('boxplots_variaveis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Boxplots salvos em 'boxplots_variaveis.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de correla√ß√£o\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISE DE CORRELA√á√ÉO ENTRE VARI√ÅVEIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calcular matriz de correla√ß√£o\n",
        "corr_matrix = df[cols_plot].corr()\n",
        "\n",
        "# Criar heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # M√°scara para mostrar apenas tri√¢ngulo inferior\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
        "            vmin=-1, vmax=1)\n",
        "plt.title('Matriz de Correla√ß√£o entre Indicadores Socioecon√¥micos', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('heatmap_correlacao.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nMatriz de correla√ß√£o:\")\n",
        "print(corr_matrix)\n",
        "\n",
        "# Interpreta√ß√£o das correla√ß√µes\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETA√á√ÉO DAS CORRELA√á√ïES (Contexto do Dom√≠nio)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extrair correla√ß√µes espec√≠ficas\n",
        "if 'Cantril ladder score' in corr_matrix.columns and 'GDP per capita. PPP (constant 2021 international $)' in corr_matrix.columns:\n",
        "    corr_cantril_gdp = corr_matrix.loc['Cantril ladder score', 'GDP per capita. PPP (constant 2021 international $)']\n",
        "    print(f\"\\n1. Cantril (Felicidade) vs PIB per capita: {corr_cantril_gdp:.3f}\")\n",
        "    if corr_cantril_gdp > 0.6:\n",
        "        print(\"   ‚Üí Forte correla√ß√£o positiva: Pa√≠ses mais ricos tendem a ter popula√ß√µes mais felizes\")\n",
        "    elif corr_cantril_gdp > 0.3:\n",
        "        print(\"   ‚Üí Correla√ß√£o moderada: Riqueza influencia felicidade, mas n√£o √© o √∫nico fator\")\n",
        "    else:\n",
        "        print(\"   ‚Üí Correla√ß√£o fraca: Dinheiro n√£o compra felicidade diretamente\")\n",
        "\n",
        "if 'Cantril ladder score' in corr_matrix.columns and 'HDI (Human Development Indicator) Value (0~1000)' in corr_matrix.columns:\n",
        "    corr_cantril_hdi = corr_matrix.loc['Cantril ladder score', 'HDI (Human Development Indicator) Value (0~1000)']\n",
        "    print(f\"\\n2. Cantril (Felicidade) vs IDH: {corr_cantril_hdi:.3f}\")\n",
        "    if corr_cantril_hdi > 0.7:\n",
        "        print(\"   ‚Üí Forte correla√ß√£o: Desenvolvimento humano est√° fortemente ligado √† felicidade\")\n",
        "    else:\n",
        "        print(\"   ‚Üí Correla√ß√£o moderada: IDH captura aspectos al√©m da felicidade subjetiva\")\n",
        "\n",
        "if 'GDP per capita. PPP (constant 2021 international $)' in corr_matrix.columns and 'HDI (Human Development Indicator) Value (0~1000)' in corr_matrix.columns:\n",
        "    corr_gdp_hdi = corr_matrix.loc['GDP per capita. PPP (constant 2021 international $)', 'HDI (Human Development Indicator) Value (0~1000)']\n",
        "    print(f\"\\n3. PIB per capita vs IDH: {corr_gdp_hdi:.3f}\")\n",
        "    if corr_gdp_hdi > 0.7:\n",
        "        print(\"   ‚Üí Forte correla√ß√£o: Riqueza econ√¥mica est√° fortemente relacionada ao desenvolvimento humano\")\n",
        "    else:\n",
        "        print(\"   ‚Üí Correla√ß√£o moderada: IDH incorpora dimens√µes al√©m da riqueza (sa√∫de, educa√ß√£o)\")\n",
        "\n",
        "if 'Intentional homicides (per 100000 people) as of 2021' in corr_matrix.columns:\n",
        "    if 'HDI (Human Development Indicator) Value (0~1000)' in corr_matrix.columns:\n",
        "        corr_homicides_hdi = corr_matrix.loc['Intentional homicides (per 100000 people) as of 2021', 'HDI (Human Development Indicator) Value (0~1000)']\n",
        "        print(f\"\\n4. Homic√≠dios vs IDH: {corr_homicides_hdi:.3f}\")\n",
        "        if corr_homicides_hdi < -0.4:\n",
        "            print(\"   ‚Üí Correla√ß√£o negativa forte: Maior desenvolvimento humano ‚Üí menor viol√™ncia\")\n",
        "        elif corr_homicides_hdi < -0.2:\n",
        "            print(\"   ‚Üí Correla√ß√£o negativa moderada: Desenvolvimento reduz viol√™ncia, mas h√° outros fatores\")\n",
        "        else:\n",
        "            print(\"   ‚Üí Correla√ß√£o fraca: Viol√™ncia pode ter causas complexas al√©m do desenvolvimento\")\n",
        "    \n",
        "    if 'GDP per capita. PPP (constant 2021 international $)' in corr_matrix.columns:\n",
        "        corr_homicides_gdp = corr_matrix.loc['Intentional homicides (per 100000 people) as of 2021', 'GDP per capita. PPP (constant 2021 international $)']\n",
        "        print(f\"\\n5. Homic√≠dios vs PIB per capita: {corr_homicides_gdp:.3f}\")\n",
        "        if corr_homicides_gdp < -0.3:\n",
        "            print(\"   ‚Üí Correla√ß√£o negativa: Pa√≠ses mais ricos tendem a ter menos homic√≠dios\")\n",
        "        else:\n",
        "            print(\"   ‚Üí Correla√ß√£o fraca: Riqueza sozinha n√£o explica completamente a viol√™ncia\")\n",
        "\n",
        "print(\"\\n‚úÖ Heatmap de correla√ß√£o salvo em 'heatmap_correlacao.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDENTIFICA√á√ÉO DA VARI√ÅVEL-ALVO\n",
        "print(\"=\" * 80)\n",
        "print(\"IDENTIFICA√á√ÉO E AN√ÅLISE DA VARI√ÅVEL-ALVO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar se existe coluna explicitamente de classe\n",
        "target_candidates = ['Country Class', 'Class', 'Target', 'Label', 'y']\n",
        "\n",
        "target_col = None\n",
        "for col in target_candidates:\n",
        "    if col in df.columns:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col:\n",
        "    print(f\"\\n‚úÖ VARI√ÅVEL-ALVO IDENTIFICADA: '{target_col}'\")\n",
        "    print(f\"\\nDistribui√ß√£o da vari√°vel-alvo:\")\n",
        "    print(df[target_col].value_counts().sort_index())\n",
        "    \n",
        "    print(f\"\\nDistribui√ß√£o percentual:\")\n",
        "    print((df[target_col].value_counts(normalize=True) * 100).sort_index())\n",
        "    \n",
        "    # Verificar balanceamento\n",
        "    counts = df[target_col].value_counts()\n",
        "    balance_ratio = counts.min() / counts.max()\n",
        "    \n",
        "    print(f\"\\nAn√°lise de balanceamento:\")\n",
        "    print(f\"  - Raz√£o entre menor e maior classe: {balance_ratio:.3f}\")\n",
        "    if balance_ratio < 0.5:\n",
        "        print(f\"  ‚ö†Ô∏è ATEN√á√ÉO: Dataset desbalanceado (raz√£o < 0.5)\")\n",
        "        print(f\"  - Recomenda√ß√£o: Usar SMOTE ou outras t√©cnicas de balanceamento\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Dataset relativamente balanceado\")\n",
        "    \n",
        "    # Visualiza√ß√£o da distribui√ß√£o da vari√°vel-alvo\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df[target_col].value_counts().sort_index().plot(kind='bar', color='steelblue', edgecolor='black')\n",
        "    plt.title('Distribui√ß√£o da Vari√°vel-Alvo (Country Class)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Classe')\n",
        "    plt.ylabel('Frequ√™ncia')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('distribuicao_target.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ Gr√°fico de distribui√ß√£o salvo em 'distribuicao_target.png'\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è ATEN√á√ÉO: Nenhuma coluna explicitamente definida como target encontrada!\")\n",
        "    print(\"Colunas dispon√≠veis:\", df.columns.tolist())\n",
        "    print(\"\\nFazendo infer√™ncia l√≥gica...\")\n",
        "    \n",
        "    # Infer√™ncia: procurar por padr√µes que indiquem vari√°vel-alvo\n",
        "    # Geralmente vari√°veis-alvo t√™m poucos valores √∫nicos e s√£o categ√≥ricas\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            unique_vals = df[col].nunique()\n",
        "            if 2 <= unique_vals <= 10:  # Prov√°vel vari√°vel de classifica√ß√£o\n",
        "                print(f\"\\nCandidato encontrado: '{col}' com {unique_vals} valores √∫nicos\")\n",
        "                print(f\"Valores: {sorted(df[col].unique())}\")\n",
        "    \n",
        "    print(\"\\n‚ö†Ô∏è DECIS√ÉO NECESS√ÅRIA: Escolha manual da vari√°vel-alvo baseada no contexto do problema\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù Justificativa da Vari√°vel-Alvo\n",
        "\n",
        "**An√°lise e Decis√£o:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Justificativa detalhada da vari√°vel-alvo\n",
        "print(\"=\" * 80)\n",
        "print(\"JUSTIFICATIVA DA ESCOLHA DA VARI√ÅVEL-ALVO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "justificativa = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "JUSTIFICATIVA DA VARI√ÅVEL-ALVO\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. VARI√ÅVEL SELECIONADA: 'Country Class'\n",
        "\n",
        "2. EVID√äNCIAS:\n",
        "   - Coluna explicitamente nomeada como \"Country Class\"\n",
        "   - Valores discretos: {sorted(df['Country Class'].unique().tolist())}\n",
        "   - N√∫mero de classes: {df['Country Class'].nunique()}\n",
        "   - Tipo de problema: CLASSIFICA√á√ÉO MULTICLASSE ({df['Country Class'].nunique()} classes)\n",
        "\n",
        "3. CONTEXTO DO PROBLEMA:\n",
        "   - O enunciado menciona: \"Ajustar um modelo de classifica√ß√£o aos dados Cantrill+GDP+HDI\"\n",
        "   - O nome do arquivo cont√©m \"FabroClassification2025OK_4classes\"\n",
        "   - Isso confirma que √© um problema de classifica√ß√£o com 4 classes\n",
        "\n",
        "4. DISTRIBUI√á√ÉO DAS CLASSES:\n",
        "\"\"\"\n",
        "\n",
        "for classe in sorted(df['Country Class'].unique()):\n",
        "    count = (df['Country Class'] == classe).sum()\n",
        "    pct = (count / len(df)) * 100\n",
        "    justificativa += f\"   - Classe {int(classe)}: {count} pa√≠ses ({pct:.1f}%)\\n\"\n",
        "\n",
        "justificativa += f\"\"\"\n",
        "5. COLUNAS ONE-HOT ENCODING:\n",
        "   - O dataset cont√©m colunas 'Class 1', 'Class 2', 'Class 3', 'Class 4'\n",
        "   - Estas s√£o representa√ß√µes one-hot encoding da vari√°vel 'Country Class'\n",
        "   - Para modelagem, usaremos 'Country Class' diretamente (mais eficiente)\n",
        "\n",
        "6. CONCLUS√ÉO:\n",
        "   ‚úÖ A vari√°vel 'Country Class' √© claramente a vari√°vel-alvo do problema de classifica√ß√£o.\n",
        "   ‚úÖ N√£o h√° ambiguidade - a escolha √© direta e justificada pelo contexto.\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(justificativa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöß 3. Prepara√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepara√ß√£o dos dados - Cria√ß√£o de c√≥pia para n√£o alterar o original\n",
        "print(\"=\" * 80)\n",
        "print(\"PREPARA√á√ÉO DOS DADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "print(\"\\n1. REMOVENDO COLUNAS REDUNDANTES E DESNECESS√ÅRIAS:\")\n",
        "print(\"   - Removendo colunas one-hot encoding (Class 1, Class 2, Class 3, Class 4)\")\n",
        "print(\"   - Removendo colunas redundantes (homicides*100, Cantril*1000)\")\n",
        "print(\"   - Removendo colunas de identifica√ß√£o que n√£o s√£o features (Country Name, Country Code)\")\n",
        "\n",
        "cols_to_drop = ['Class 4', 'Class 3', 'Class 2', 'Class 1',\n",
        "                'Intentional homicides/100k  x 100',\n",
        "                'Cantril ladder score*1000',\n",
        "                'Country Name', 'Country Code']\n",
        "\n",
        "# Verificar quais colunas existem antes de remover\n",
        "cols_to_drop_existing = [col for col in cols_to_drop if col in df_clean.columns]\n",
        "df_clean = df_clean.drop(columns=cols_to_drop_existing)\n",
        "\n",
        "print(f\"   ‚úÖ {len(cols_to_drop_existing)} colunas removidas\")\n",
        "print(f\"   Colunas restantes: {df_clean.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tratamento de valores ausentes\n",
        "print(\"\\n2. TRATAMENTO DE VALORES AUSENTES:\")\n",
        "\n",
        "missing_before = df_clean.isnull().sum().sum()\n",
        "print(f\"   Valores ausentes antes do tratamento: {missing_before}\")\n",
        "\n",
        "if missing_before > 0:\n",
        "    print(\"\\n   Estrat√©gia escolhida:\")\n",
        "    print(\"   - Para vari√°veis num√©ricas: imputa√ß√£o com mediana (robusta a outliers)\")\n",
        "    print(\"   - Para vari√°veis categ√≥ricas: imputa√ß√£o com moda\")\n",
        "    \n",
        "    # Separar num√©ricas e categ√≥ricas\n",
        "    numeric_cols_clean = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if 'Country Class' in numeric_cols_clean:\n",
        "        numeric_cols_clean.remove('Country Class')\n",
        "    \n",
        "    categorical_cols_clean = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    # Imputar num√©ricas com mediana\n",
        "    for col in numeric_cols_clean:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            print(f\"   - {col}: {df_clean[col].isnull().sum()} valores imputados com mediana={median_val:.2f}\")\n",
        "    \n",
        "    # Imputar categ√≥ricas com moda\n",
        "    for col in categorical_cols_clean:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            mode_val = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'Unknown'\n",
        "            df_clean[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"   - {col}: valores imputados com moda='{mode_val}'\")\n",
        "    \n",
        "    missing_after = df_clean.isnull().sum().sum()\n",
        "    print(f\"\\n   ‚úÖ Valores ausentes ap√≥s tratamento: {missing_after}\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Nenhum valor ausente encontrado - nenhum tratamento necess√°rio\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encoding de vari√°veis categ√≥ricas\n",
        "print(\"\\n3. ENCODING DE VARI√ÅVEIS CATEG√ìRICAS:\")\n",
        "\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    print(f\"   Vari√°veis categ√≥ricas encontradas: {categorical_cols}\")\n",
        "    \n",
        "    # Verificar se 'World regions according to OWID' existe\n",
        "    if 'World regions according to OWID' in categorical_cols:\n",
        "        print(\"\\n   Estrat√©gia: Label Encoding para 'World regions according to OWID'\")\n",
        "        print(\"   Justificativa: Vari√°vel ordinal pode ser codificada numericamente\")\n",
        "        \n",
        "        le = LabelEncoder()\n",
        "        df_clean['World regions according to OWID_encoded'] = le.fit_transform(df_clean['World regions according to OWID'])\n",
        "        df_clean = df_clean.drop(columns=['World regions according to OWID'])\n",
        "        \n",
        "        print(f\"   ‚úÖ Encoding realizado. Classes mapeadas:\")\n",
        "        for i, region in enumerate(le.classes_):\n",
        "            print(f\"      {i}: {region}\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Nenhuma vari√°vel categ√≥rica encontrada (ou j√° foram removidas)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separa√ß√£o de features e target\n",
        "print(\"\\n4. SEPARA√á√ÉO DE FEATURES E TARGET:\")\n",
        "\n",
        "# Definir features (todas as colunas exceto a target)\n",
        "feature_cols = [col for col in df_clean.columns if col != 'Country Class']\n",
        "X = df_clean[feature_cols]\n",
        "y_original = df_clean['Country Class'].astype(int)  # Valores originais (1-4)\n",
        "\n",
        "# IMPORTANTE: Mapear classes de 1-4 para 0-3 (requerido por XGBoost e alguns algoritmos)\n",
        "# Criar mapeamento para manter refer√™ncia das classes originais\n",
        "class_mapping = {1: 0, 2: 1, 3: 2, 4: 3}\n",
        "y = y_original.map(class_mapping).astype(int)\n",
        "\n",
        "print(f\"   Features selecionadas ({len(feature_cols)}): {feature_cols}\")\n",
        "print(f\"   Target: Country Class\")\n",
        "print(f\"   Shape de X: {X.shape}\")\n",
        "print(f\"   Shape de y: {y.shape}\")\n",
        "print(f\"\\n   Classes originais (1-4) mapeadas para (0-3):\")\n",
        "print(f\"   - Classe original 1 ‚Üí Classe 0\")\n",
        "print(f\"   - Classe original 2 ‚Üí Classe 1\")\n",
        "print(f\"   - Classe original 3 ‚Üí Classe 2\")\n",
        "print(f\"   - Classe original 4 ‚Üí Classe 3\")\n",
        "print(f\"\\n   Distribui√ß√£o de y (ap√≥s mapeamento): {y.value_counts().sort_index().to_dict()}\")\n",
        "print(f\"   Valores √∫nicos em y: {sorted(y.unique())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar mapeamento reverso para exibi√ß√£o (0-3 ‚Üí 1-4)\n",
        "reverse_class_mapping = {0: 1, 1: 2, 2: 3, 3: 4}\n",
        "\n",
        "# Separa√ß√£o treino/teste\n",
        "print(\"\\n5. SEPARA√á√ÉO TREINO/TESTE:\")\n",
        "\n",
        "print(\"   Estrat√©gia: Stratified Split (80% treino, 20% teste)\")\n",
        "print(\"   Justificativa:\")\n",
        "print(\"     - Stratified garante que a propor√ß√£o de classes seja mantida em ambos os conjuntos\")\n",
        "print(\"     - 80/20 √© um split padr√£o que oferece bom equil√≠brio entre treino e valida√ß√£o\")\n",
        "print(\"     - random_state=42 para reprodutibilidade\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n   ‚úÖ Separa√ß√£o realizada:\")\n",
        "print(f\"   - Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"   - Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n   Distribui√ß√£o de classes no conjunto de TREINO (classes mapeadas 0-3):\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\n   Distribui√ß√£o de classes no conjunto de TESTE (classes mapeadas 0-3):\")\n",
        "print(y_test.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avalia√ß√£o de balanceamento\n",
        "print(\"\\n6. AVALIA√á√ÉO DE BALANCEAMENTO:\")\n",
        "\n",
        "train_counts = y_train.value_counts().sort_index()\n",
        "balance_ratio = train_counts.min() / train_counts.max()\n",
        "\n",
        "print(f\"   Raz√£o entre menor e maior classe (treino): {balance_ratio:.3f}\")\n",
        "\n",
        "if balance_ratio < 0.5:\n",
        "    print(f\"   ‚ö†Ô∏è Dataset desbalanceado detectado!\")\n",
        "    print(f\"   Aplicando SMOTE para balancear o conjunto de treino...\")\n",
        "    \n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "    \n",
        "    print(f\"   ‚úÖ SMOTE aplicado:\")\n",
        "    print(f\"   - Antes: {X_train.shape[0]} amostras\")\n",
        "    print(f\"   - Depois: {X_train_balanced.shape[0]} amostras\")\n",
        "    print(f\"   - Distribui√ß√£o ap√≥s SMOTE:\")\n",
        "    print(y_train_balanced.value_counts().sort_index())\n",
        "    \n",
        "    # Atualizar vari√°veis\n",
        "    X_train = X_train_balanced\n",
        "    y_train = y_train_balanced\n",
        "else:\n",
        "    print(f\"   ‚úÖ Dataset relativamente balanceado - SMOTE n√£o necess√°rio\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normaliza√ß√£o/Padroniza√ß√£o\n",
        "print(\"\\n7. NORMALIZA√á√ÉO/PADRONIZA√á√ÉO:\")\n",
        "\n",
        "print(\"   Estrat√©gia: StandardScaler (padroniza√ß√£o Z-score)\")\n",
        "print(\"   Justificativa:\")\n",
        "print(\"     - Vari√°veis t√™m escalas muito diferentes (ex: GDP em dezenas de milhares, HDI em centenas)\")\n",
        "print(\"     - Padroniza√ß√£o melhora performance de algoritmos sens√≠veis √† escala (Logistic Regression)\")\n",
        "print(\"     - Tree-based models (RF, XGBoost) n√£o s√£o sens√≠veis, mas padroniza√ß√£o n√£o prejudica\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converter de volta para DataFrame para manter nomes das colunas\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index if hasattr(X_train, 'index') else None)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
        "\n",
        "print(f\"   ‚úÖ Padroniza√ß√£o realizada\")\n",
        "print(f\"   M√©dias ap√≥s padroniza√ß√£o (treino): {X_train_scaled.mean().round(3).to_dict()}\")\n",
        "print(f\"   Desvios padr√£o ap√≥s padroniza√ß√£o (treino): {X_train_scaled.std().round(3).to_dict()}\")\n",
        "\n",
        "# Usar dados padronizados\n",
        "X_train = X_train_scaled\n",
        "X_test = X_test_scaled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 4. Ajuste de Modelos de Classifica√ß√£o\n",
        "\n",
        "### üìö O que significa \"Ajustar um Modelo de Classifica√ß√£o aos Dados\"?\n",
        "\n",
        "**Ajustar um modelo de classifica√ß√£o** significa **treinar um algoritmo de Machine Learning** para aprender padr√µes nos dados que permitam prever a classe (categoria) de novos exemplos.\n",
        "\n",
        "#### Conceitos Fundamentais:\n",
        "\n",
        "1. **Treinamento (Training)**: \n",
        "   - O modelo \"aprende\" a partir de exemplos conhecidos (dados de treino)\n",
        "   - Identifica padr√µes e rela√ß√µes entre as vari√°veis preditoras (features) e a vari√°vel-alvo (target)\n",
        "   - No nosso caso: aprende a relacionar Cantril, GDP, HDI e Homic√≠dios com a Classifica√ß√£o do pa√≠s\n",
        "\n",
        "2. **O que o modelo aprende**:\n",
        "   - Quais valores de PIB, IDH, Felicidade e Homic√≠dios s√£o t√≠picos de cada classe\n",
        "   - Regras de decis√£o para classificar um pa√≠s baseado nos seus indicadores\n",
        "   - Exemplo: \"Se IDH > 900 e PIB > 50.000, ent√£o provavelmente √© Classe 1\"\n",
        "\n",
        "3. **Objetivo**:\n",
        "   - Poder prever a classe de um pa√≠s NOVO (que n√£o estava no dataset de treino)\n",
        "   - Baseado apenas nos seus indicadores (Cantril, GDP, HDI, Homic√≠dios)\n",
        "\n",
        "4. **Valida√ß√£o**:\n",
        "   - Testamos o modelo em dados que ele nunca viu (conjunto de teste)\n",
        "   - Medimos a acur√°cia, precis√£o, recall, etc.\n",
        "   - Isso garante que o modelo generaliza bem (n√£o apenas \"decorou\" os dados de treino)\n",
        "\n",
        "#### No contexto deste trabalho:\n",
        "- **Vari√°veis Preditoras (Features)**: Cantril, GDP, HDI, Homic√≠dios, Regi√£o\n",
        "- **Vari√°vel-Alvo (Target)**: Country Class (1, 2, 3 ou 4)\n",
        "- **Objetivo**: Criar um modelo que, dado os indicadores de um pa√≠s, preveja sua classe de desenvolvimento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explica√ß√£o Pr√°tica: Como funciona o Ajuste de um Modelo de Classifica√ß√£o\n",
        "print(\"=\" * 80)\n",
        "print(\"EXPLICA√á√ÉO PR√ÅTICA: COMO FUNCIONA O AJUSTE DE UM MODELO DE CLASSIFICA√á√ÉO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "explicacao = \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "COMO FUNCIONA O AJUSTE DE UM MODELO DE CLASSIFICA√á√ÉO\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "üìä PASSO A PASSO:\n",
        "\n",
        "1. DADOS DE TREINO (80% dos dados):\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Pa√≠s    ‚îÇ Cantril ‚îÇ GDP    ‚îÇ IDH  ‚îÇ Homic√≠dios ‚îÇ Classe    ‚îÇ\n",
        "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "   ‚îÇ Noruega ‚îÇ  7.30   ‚îÇ 90.470 ‚îÇ  966 ‚îÇ    0.54    ‚îÇ    1      ‚îÇ ‚Üê Exemplo\n",
        "   ‚îÇ Brasil  ‚îÇ  6.27   ‚îÇ 19.018 ‚îÇ  760 ‚îÇ   22.38    ‚îÇ    2      ‚îÇ ‚Üê Exemplo\n",
        "   ‚îÇ ...     ‚îÇ   ...   ‚îÇ  ...   ‚îÇ ...  ‚îÇ    ...     ‚îÇ   ...     ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "   \n",
        "   O modelo v√™ MUITOS exemplos como estes e aprende padr√µes:\n",
        "   - \"Pa√≠ses com IDH > 900 e PIB > 50.000 geralmente s√£o Classe 1\"\n",
        "   - \"Pa√≠ses com IDH < 600 geralmente s√£o Classe 3 ou 4\"\n",
        "   - etc.\n",
        "\n",
        "2. PROCESSO DE APRENDIZADO:\n",
        "   \n",
        "   O modelo cria \"regras de decis√£o\" internas:\n",
        "   \n",
        "   Exemplo (simplificado):\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ SE IDH > 900 E PIB > 50.000                            ‚îÇ\n",
        "   ‚îÇ   ENT√ÉO Classe = 1 (Desenvolvido)                      ‚îÇ\n",
        "   ‚îÇ                                                         ‚îÇ\n",
        "   ‚îÇ SEN√ÉO SE IDH > 750 E PIB > 20.000                     ‚îÇ\n",
        "   ‚îÇ   ENT√ÉO Classe = 2 (Em Desenvolvimento)               ‚îÇ\n",
        "   ‚îÇ                                                         ‚îÇ\n",
        "   ‚îÇ SEN√ÉO SE IDH > 600                                     ‚îÇ\n",
        "   ‚îÇ   ENT√ÉO Classe = 3 (Desenvolvimento M√©dio)            ‚îÇ\n",
        "   ‚îÇ                                                         ‚îÇ\n",
        "   ‚îÇ SEN√ÉO                                                  ‚îÇ\n",
        "   ‚îÇ   ENT√ÉO Classe = 4 (Baixo Desenvolvimento)             ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "   \n",
        "   (Na pr√°tica, os modelos s√£o muito mais complexos e consideram\n",
        "    m√∫ltiplas vari√°veis simultaneamente)\n",
        "\n",
        "3. DADOS DE TESTE (20% dos dados - que o modelo NUNCA VIU):\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Pa√≠s    ‚îÇ Cantril ‚îÇ GDP    ‚îÇ IDH  ‚îÇ Homic√≠dios ‚îÇ Classe    ‚îÇ\n",
        "   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "   ‚îÇ Su√©cia  ‚îÇ  7.34   ‚îÇ 63.115 ‚îÇ  952 ‚îÇ    1.08    ‚îÇ    ?      ‚îÇ\n",
        "   ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ      ‚îÇ            ‚îÇ           ‚îÇ\n",
        "   ‚îÇ O modelo faz uma PREDI√á√ÉO baseada nas regras aprendidas:    ‚îÇ\n",
        "   ‚îÇ ‚Üí IDH=952 (>900) E PIB=63.115 (>50.000)                    ‚îÇ\n",
        "   ‚îÇ ‚Üí PREDI√á√ÉO: Classe 1                                       ‚îÇ\n",
        "   ‚îÇ ‚Üí CLASSE REAL: 1 ‚úÖ (Correto!)                             ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "4. AVALIA√á√ÉO:\n",
        "   - Testamos o modelo em TODOS os pa√≠ses do conjunto de teste\n",
        "   - Contamos quantas predi√ß√µes foram corretas (Accuracy)\n",
        "   - Analisamos erros por classe (Precision, Recall, F1-Score)\n",
        "   - Isso nos diz se o modelo \"aprendeu bem\" ou n√£o\n",
        "\n",
        "5. RESULTADO FINAL:\n",
        "   - Um modelo \"treinado\" que pode classificar NOVOS pa√≠ses\n",
        "   - Exemplo: Se voc√™ tiver dados de um pa√≠s novo:\n",
        "     * Cantril = 6.5, GDP = 25.000, IDH = 800, Homic√≠dios = 3.0\n",
        "     * O modelo prev√™: Classe 2\n",
        "     * (Mesmo que esse pa√≠s nunca tenha estado no dataset original)\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "üéØ RESUMO:\n",
        "   \"Ajustar um modelo\" = Treinar um algoritmo para aprender padr√µes\n",
        "   nos dados e poder fazer predi√ß√µes em dados novos.\n",
        "\n",
        "   √â como ensinar uma crian√ßa a reconhecer animais:\n",
        "   - Mostramos muitos exemplos (dados de treino)\n",
        "   - Ela aprende padr√µes (4 patas + rabo = cachorro)\n",
        "   - Testamos com animais novos (dados de teste)\n",
        "   - Se acertar bem, ela \"aprendeu\" (modelo est√° ajustado)\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(explicacao)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explica√ß√£o dos Modelos que ser√£o Ajustados\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TIPOS DE MODELOS DE CLASSIFICA√á√ÉO QUE SER√ÉO AJUSTADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "modelos_explicacao = \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "TIPOS DE MODELOS DE CLASSIFICA√á√ÉO\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "Vamos ajustar 3 modelos diferentes, cada um com uma abordagem √∫nica:\n",
        "\n",
        "1. üìà LOGISTIC REGRESSION (Regress√£o Log√≠stica):\n",
        "   \n",
        "   Como funciona:\n",
        "   - Cria uma \"fronteira de decis√£o\" linear no espa√ßo das vari√°veis\n",
        "   - Calcula probabilidades de cada classe usando uma fun√ß√£o sigmoide\n",
        "   - Exemplo: P(Classe=1) = 1 / (1 + e^(-(Œ≤‚ÇÄ + Œ≤‚ÇÅ√óIDH + Œ≤‚ÇÇ√óPIB + ...)))\n",
        "   \n",
        "   Vantagens:\n",
        "   - Interpret√°vel (podemos ver os coeficientes Œ≤)\n",
        "   - R√°pido de treinar\n",
        "   - Boa para rela√ß√µes lineares\n",
        "   \n",
        "   Limita√ß√µes:\n",
        "   - Assume rela√ß√µes lineares entre vari√°veis\n",
        "   - Pode n√£o capturar padr√µes complexos\n",
        "\n",
        "2. üå≥ RANDOM FOREST (Floresta Aleat√≥ria):\n",
        "   \n",
        "   Como funciona:\n",
        "   - Cria MUITAS √°rvores de decis√£o (ex: 100 √°rvores)\n",
        "   - Cada √°rvore vota em uma classe\n",
        "   - A classe mais votada √© a predi√ß√£o final\n",
        "   \n",
        "   Exemplo de uma √°rvore:\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ IDH > 900?                          ‚îÇ\n",
        "   ‚îÇ    ‚îú‚îÄ SIM ‚Üí PIB > 50.000?          ‚îÇ\n",
        "   ‚îÇ    ‚îÇ    ‚îú‚îÄ SIM ‚Üí Classe 1          ‚îÇ\n",
        "   ‚îÇ    ‚îÇ    ‚îî‚îÄ N√ÉO ‚Üí Classe 2          ‚îÇ\n",
        "   ‚îÇ    ‚îî‚îÄ N√ÉO ‚Üí Homic√≠dios > 10?       ‚îÇ\n",
        "   ‚îÇ         ‚îú‚îÄ SIM ‚Üí Classe 3           ‚îÇ\n",
        "   ‚îÇ         ‚îî‚îÄ N√ÉO ‚Üí Classe 4           ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "   \n",
        "   Vantagens:\n",
        "   - Captura rela√ß√µes n√£o-lineares\n",
        "   - N√£o precisa normaliza√ß√£o\n",
        "   - Mostra import√¢ncia das vari√°veis\n",
        "   \n",
        "   Limita√ß√µes:\n",
        "   - Menos interpret√°vel que Logistic Regression\n",
        "   - Pode ser mais lento com muitos dados\n",
        "\n",
        "3. üöÄ XGBOOST (Extreme Gradient Boosting):\n",
        "   \n",
        "   Como funciona:\n",
        "   - Cria √°rvores sequencialmente, onde cada √°rvore corrige os erros da anterior\n",
        "   - √â um \"ensemble\" que combina m√∫ltiplas √°rvores fracas em um modelo forte\n",
        "   - Usa otimiza√ß√£o avan√ßada (gradient boosting)\n",
        "   \n",
        "   Analogia:\n",
        "   - Como um estudante que faz v√°rias provas\n",
        "   - A cada prova, foca nos t√≥picos que errou na anterior\n",
        "   - No final, domina todos os t√≥picos\n",
        "   \n",
        "   Vantagens:\n",
        "   - Geralmente o mais preciso\n",
        "   - Captura padr√µes complexos\n",
        "   - Tem regulariza√ß√£o para evitar overfitting\n",
        "   \n",
        "   Limita√ß√µes:\n",
        "   - Mais complexo de interpretar\n",
        "   - Pode ser mais lento de treinar\n",
        "   - Requer mais ajuste de hiperpar√¢metros\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "üî¨ POR QUE TESTAR M√öLTIPLOS MODELOS?\n",
        "\n",
        "1. Diferentes modelos capturam diferentes padr√µes\n",
        "2. N√£o sabemos a priori qual ser√° melhor para nossos dados\n",
        "3. Compara√ß√£o nos permite escolher o melhor\n",
        "4. Cada modelo tem trade-offs (precis√£o vs interpretabilidade)\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(modelos_explicacao)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explica√ß√£o T√©cnica: O que acontece durante o \"Ajuste\"\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPLICA√á√ÉO T√âCNICA: O QUE ACONTECE DURANTE O AJUSTE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tecnica_explicacao = \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "O PROCESSO T√âCNICO DE AJUSTE\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "Quando chamamos model.fit(X_train, y_train), o que acontece internamente:\n",
        "\n",
        "1. INICIALIZA√á√ÉO:\n",
        "   - O modelo come√ßa com par√¢metros aleat√≥rios ou padr√£o\n",
        "   - Exemplo (Logistic Regression): Œ≤‚ÇÄ=0, Œ≤‚ÇÅ=0, Œ≤‚ÇÇ=0, ...\n",
        "   - Neste ponto, o modelo \"n√£o sabe nada\" e faz predi√ß√µes aleat√≥rias\n",
        "\n",
        "2. ITERA√á√ÉO (Loop de Aprendizado):\n",
        "   \n",
        "   Para cada itera√ß√£o:\n",
        "   \n",
        "   a) PREDI√á√ÉO:\n",
        "      - O modelo usa os par√¢metros atuais para prever as classes\n",
        "      - Compara predi√ß√µes com valores reais\n",
        "      - Calcula o ERRO (quantas predi√ß√µes est√£o erradas)\n",
        "   \n",
        "   b) AJUSTE:\n",
        "      - O modelo calcula como ajustar os par√¢metros para reduzir o erro\n",
        "      - Usa algoritmos de otimiza√ß√£o (ex: gradient descent)\n",
        "      - Atualiza os par√¢metros: Œ≤‚ÇÄ, Œ≤‚ÇÅ, Œ≤‚ÇÇ, ...\n",
        "   \n",
        "   c) AVALIA√á√ÉO:\n",
        "      - Verifica se o erro diminuiu\n",
        "      - Se sim, continua; se n√£o, tenta outra abordagem\n",
        "   \n",
        "   Exemplo simplificado:\n",
        "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   ‚îÇ Itera√ß√£o 1: Erro = 45% (muito alto)                ‚îÇ\n",
        "   ‚îÇ ‚Üí Ajusta par√¢metros                                 ‚îÇ\n",
        "   ‚îÇ                                                     ‚îÇ\n",
        "   ‚îÇ Itera√ß√£o 2: Erro = 30% (melhorou!)                 ‚îÇ\n",
        "   ‚îÇ ‚Üí Ajusta par√¢metros novamente                       ‚îÇ\n",
        "   ‚îÇ                                                     ‚îÇ\n",
        "   ‚îÇ Itera√ß√£o 3: Erro = 20% (melhorou mais!)           ‚îÇ\n",
        "   ‚îÇ ‚Üí Continua ajustando...                            ‚îÇ\n",
        "   ‚îÇ                                                     ‚îÇ\n",
        "   ‚îÇ Itera√ß√£o N: Erro = 15% (converg√™ncia)              ‚îÇ\n",
        "   ‚îÇ ‚Üí Para o treinamento                                ‚îÇ\n",
        "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "3. CONVERG√äNCIA:\n",
        "   - O modelo para quando:\n",
        "     * O erro n√£o diminui mais (converg√™ncia)\n",
        "     * Atingiu n√∫mero m√°ximo de itera√ß√µes\n",
        "     * O erro est√° abaixo de um threshold\n",
        "   \n",
        "4. RESULTADO:\n",
        "   - Par√¢metros \"√≥timos\" encontrados\n",
        "   - Modelo pronto para fazer predi√ß√µes em dados novos\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "üìä ANALOGIA DO MUNDO REAL:\n",
        "\n",
        "√â como ajustar uma receita de bolo:\n",
        "- Come√ßamos com uma receita inicial (par√¢metros iniciais)\n",
        "- Fazemos um bolo e provamos (predi√ß√£o)\n",
        "- Se n√£o ficou bom, ajustamos ingredientes (ajuste de par√¢metros)\n",
        "- Repetimos at√© o bolo ficar perfeito (converg√™ncia)\n",
        "- Quando est√° bom, temos a receita final (modelo ajustado)\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "üéØ NO NOSSO CASO ESPEC√çFICO:\n",
        "\n",
        "Quando ajustamos o modelo aos dados Cantril+GDP+HDI:\n",
        "\n",
        "1. O modelo aprende pesos/import√¢ncias para cada vari√°vel:\n",
        "   - Exemplo: IDH tem peso 0.4, PIB tem peso 0.3, Cantril tem peso 0.2, etc.\n",
        "\n",
        "2. Aprende thresholds (limiares) de decis√£o:\n",
        "   - Exemplo: Se IDH > 900 ‚Üí alta probabilidade de Classe 1\n",
        "\n",
        "3. Aprende intera√ß√µes entre vari√°veis:\n",
        "   - Exemplo: Pa√≠ses com IDH alto E PIB alto ‚Üí Classe 1\n",
        "   - Exemplo: Pa√≠ses com IDH baixo E homic√≠dios altos ‚Üí Classe 4\n",
        "\n",
        "4. Cria regras de classifica√ß√£o que funcionam bem nos dados de treino\n",
        "   E (esperamos) tamb√©m funcionar√£o bem em dados novos\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(tecnica_explicacao)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configura√ß√£o para valida√ß√£o cruzada\n",
        "print(\"=\" * 80)\n",
        "print(\"AJUSTE DE MODELOS DE CLASSIFICA√á√ÉO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Configurar valida√ß√£o cruzada estratificada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "print(f\"\\nValida√ß√£o cruzada: {cv.n_splits}-fold estratificado\")\n",
        "\n",
        "# Dicion√°rio para armazenar resultados\n",
        "resultados = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODELO 1: Logistic Regression\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# Valida√ß√£o cruzada\n",
        "print(\"\\nExecutando valida√ß√£o cruzada...\")\n",
        "cv_scores_lr = cross_val_score(lr, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "print(f\"Accuracy m√©dia (CV): {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std() * 2:.4f})\")\n",
        "\n",
        "# Treinar no conjunto completo de treino\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predi√ß√µes\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_pred_proba_lr = lr.predict_proba(X_test)\n",
        "\n",
        "# M√©tricas\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "prec_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
        "rec_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
        "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
        "\n",
        "resultados['Logistic Regression'] = {\n",
        "    'model': lr,\n",
        "    'accuracy': acc_lr,\n",
        "    'precision': prec_lr,\n",
        "    'recall': rec_lr,\n",
        "    'f1': f1_lr,\n",
        "    'cv_scores': cv_scores_lr,\n",
        "    'y_pred': y_pred_lr,\n",
        "    'y_pred_proba': y_pred_proba_lr\n",
        "}\n",
        "\n",
        "print(f\"\\nM√©tricas no conjunto de teste:\")\n",
        "print(f\"  Accuracy: {acc_lr:.4f}\")\n",
        "print(f\"  Precision (weighted): {prec_lr:.4f}\")\n",
        "print(f\"  Recall (weighted): {rec_lr:.4f}\")\n",
        "print(f\"  F1-score (weighted): {f1_lr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODELO 2: Random Forest\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 2: RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "\n",
        "# Valida√ß√£o cruzada\n",
        "print(\"\\nExecutando valida√ß√£o cruzada...\")\n",
        "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "print(f\"Accuracy m√©dia (CV): {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")\n",
        "\n",
        "# Treinar no conjunto completo de treino\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predi√ß√µes\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_pred_proba_rf = rf.predict_proba(X_test)\n",
        "\n",
        "# M√©tricas\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "prec_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "rec_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "resultados['Random Forest'] = {\n",
        "    'model': rf,\n",
        "    'accuracy': acc_rf,\n",
        "    'precision': prec_rf,\n",
        "    'recall': rec_rf,\n",
        "    'f1': f1_rf,\n",
        "    'cv_scores': cv_scores_rf,\n",
        "    'y_pred': y_pred_rf,\n",
        "    'y_pred_proba': y_pred_proba_rf\n",
        "}\n",
        "\n",
        "print(f\"\\nM√©tricas no conjunto de teste:\")\n",
        "print(f\"  Accuracy: {acc_rf:.4f}\")\n",
        "print(f\"  Precision (weighted): {prec_rf:.4f}\")\n",
        "print(f\"  Recall (weighted): {rec_rf:.4f}\")\n",
        "print(f\"  F1-score (weighted): {f1_rf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODELO 3: XGBoost (Gradient Boosting)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODELO 3: XGBOOST (GRADIENT BOOSTING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, n_estimators=100, max_depth=5, learning_rate=0.1)\n",
        "\n",
        "# Valida√ß√£o cruzada\n",
        "print(\"\\nExecutando valida√ß√£o cruzada...\")\n",
        "cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "print(f\"Accuracy m√©dia (CV): {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std() * 2:.4f})\")\n",
        "\n",
        "# Treinar no conjunto completo de treino\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predi√ß√µes\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_pred_proba_xgb = xgb_model.predict_proba(X_test)\n",
        "\n",
        "# M√©tricas\n",
        "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "prec_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
        "rec_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
        "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
        "\n",
        "resultados['XGBoost'] = {\n",
        "    'model': xgb_model,\n",
        "    'accuracy': acc_xgb,\n",
        "    'precision': prec_xgb,\n",
        "    'recall': rec_xgb,\n",
        "    'f1': f1_xgb,\n",
        "    'cv_scores': cv_scores_xgb,\n",
        "    'y_pred': y_pred_xgb,\n",
        "    'y_pred_proba': y_pred_proba_xgb\n",
        "}\n",
        "\n",
        "print(f\"\\nM√©tricas no conjunto de teste:\")\n",
        "print(f\"  Accuracy: {acc_xgb:.4f}\")\n",
        "print(f\"  Precision (weighted): {prec_xgb:.4f}\")\n",
        "print(f\"  Recall (weighted): {rec_xgb:.4f}\")\n",
        "print(f\"  F1-score (weighted): {f1_xgb:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o de modelos\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARA√á√ÉO DE MODELOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparacao = pd.DataFrame({\n",
        "    'Modelo': list(resultados.keys()),\n",
        "    'Accuracy': [resultados[m]['accuracy'] for m in resultados.keys()],\n",
        "    'Precision': [resultados[m]['precision'] for m in resultados.keys()],\n",
        "    'Recall': [resultados[m]['recall'] for m in resultados.keys()],\n",
        "    'F1-Score': [resultados[m]['f1'] for m in resultados.keys()],\n",
        "    'CV Accuracy (m√©dia)': [resultados[m]['cv_scores'].mean() for m in resultados.keys()],\n",
        "    'CV Accuracy (std)': [resultados[m]['cv_scores'].std() for m in resultados.keys()]\n",
        "})\n",
        "\n",
        "comparacao = comparacao.sort_values('F1-Score', ascending=False)\n",
        "print(\"\\nTabela comparativa (ordenada por F1-Score):\")\n",
        "print(comparacao.to_string(index=False))\n",
        "\n",
        "# Identificar melhor modelo\n",
        "melhor_modelo_nome = comparacao.iloc[0]['Modelo']\n",
        "melhor_modelo = resultados[melhor_modelo_nome]['model']\n",
        "\n",
        "print(f\"\\nüèÜ MELHOR MODELO: {melhor_modelo_nome}\")\n",
        "print(f\"   F1-Score: {comparacao.iloc[0]['F1-Score']:.4f}\")\n",
        "print(f\"   Accuracy: {comparacao.iloc[0]['Accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrizes de confus√£o\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MATRIZES DE CONFUS√ÉO\")\n",
        "print(\"=\"*80)\n",
        "print(\"Nota: Classes mostradas como originais (1-4) para melhor interpreta√ß√£o\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (nome, res) in enumerate(resultados.items()):\n",
        "    cm = confusion_matrix(y_test, res['y_pred'])\n",
        "    # Mapear labels para classes originais (0-3 ‚Üí 1-4)\n",
        "    class_labels = [reverse_class_mapping[i] for i in sorted(y_test.unique())]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=class_labels, yticklabels=class_labels)\n",
        "    axes[idx].set_title(f'{nome}\\nAccuracy: {res[\"accuracy\"]:.3f}', fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predito (Classe Original)')\n",
        "    axes[idx].set_ylabel('Real (Classe Original)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('matrizes_confusao.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Matrizes de confus√£o salvas em 'matrizes_confusao.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Relat√≥rios de classifica√ß√£o detalhados\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RELAT√ìRIOS DE CLASSIFICA√á√ÉO DETALHADOS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Nota: Classes mostradas como originais (1-4) para melhor interpreta√ß√£o\\n\")\n",
        "\n",
        "for nome, res in resultados.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{nome.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    # Mapear labels para classes originais\n",
        "    class_names = [f'Classe {reverse_class_mapping[i]}' for i in sorted(y_test.unique())]\n",
        "    print(classification_report(y_test, res['y_pred'], \n",
        "                                target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curve e AUC (One-vs-Rest para multiclasse)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CURVAS ROC E AUC\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from itertools import cycle\n",
        "\n",
        "# Binarizar as classes para One-vs-Rest\n",
        "y_test_bin = label_binarize(y_test, classes=sorted(y_test.unique()))\n",
        "n_classes = y_test_bin.shape[1]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (nome, res) in enumerate(resultados.items()):\n",
        "    y_pred_proba = res['y_pred_proba']\n",
        "    \n",
        "    # Calcular ROC para cada classe\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    \n",
        "    # Calcular m√©dia macro\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "    mean_tpr /= n_classes\n",
        "    \n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "    \n",
        "    # Plot\n",
        "    axes[idx].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "                   label=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.3f})',\n",
        "                   linewidth=2)\n",
        "    \n",
        "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        # Mapear para classe original para exibi√ß√£o\n",
        "        original_class = reverse_class_mapping[sorted(y_test.unique())[i]]\n",
        "        axes[idx].plot(fpr[i], tpr[i], color=color, lw=1.5, alpha=0.7,\n",
        "                      label=f'Classe {original_class} (AUC = {roc_auc[i]:.3f})')\n",
        "    \n",
        "    axes[idx].plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.5)\n",
        "    axes[idx].set_xlim([0.0, 1.0])\n",
        "    axes[idx].set_ylim([0.0, 1.05])\n",
        "    axes[idx].set_xlabel('Taxa de Falsos Positivos')\n",
        "    axes[idx].set_ylabel('Taxa de Verdadeiros Positivos')\n",
        "    axes[idx].set_title(f'{nome}\\nROC Curves', fontweight='bold')\n",
        "    axes[idx].legend(loc=\"lower right\", fontsize=8)\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Curvas ROC salvas em 'roc_curves.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù An√°lise Comparativa dos Modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise textual dos resultados\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISE COMPARATIVA DOS MODELOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "analise = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "AN√ÅLISE COMPARATIVA DOS MODELOS DE CLASSIFICA√á√ÉO\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. RESUMO DAS PERFORMANCES:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for nome, res in resultados.items():\n",
        "    analise += f\"\"\"\n",
        "   {nome}:\n",
        "   - Accuracy: {res['accuracy']:.4f}\n",
        "   - Precision (weighted): {res['precision']:.4f}\n",
        "   - Recall (weighted): {res['recall']:.4f}\n",
        "   - F1-Score (weighted): {res['f1']:.4f}\n",
        "   - CV Accuracy (m√©dia ¬± std): {res['cv_scores'].mean():.4f} ¬± {res['cv_scores'].std():.4f}\n",
        "\"\"\"\n",
        "\n",
        "analise += f\"\"\"\n",
        "\n",
        "2. MELHOR MODELO IDENTIFICADO: {melhor_modelo_nome}\n",
        "\n",
        "3. JUSTIFICATIVA DA ESCOLHA:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Comparar modelos\n",
        "if melhor_modelo_nome == 'XGBoost':\n",
        "    analise += \"\"\"\n",
        "   O XGBoost apresentou a melhor performance geral, o que √© esperado porque:\n",
        "   - Algoritmos de gradient boosting s√£o poderosos para problemas de classifica√ß√£o\n",
        "   - XGBoost tem mecanismos de regulariza√ß√£o que previnem overfitting\n",
        "   - Boa capacidade de capturar rela√ß√µes n√£o-lineares entre features\n",
        "   - Performance superior em F1-Score, que √© uma m√©trica balanceada\n",
        "\"\"\"\n",
        "elif melhor_modelo_nome == 'Random Forest':\n",
        "    analise += \"\"\"\n",
        "   O Random Forest apresentou a melhor performance, o que √© esperado porque:\n",
        "   - Ensemble de √°rvores reduz vari√¢ncia e melhora generaliza√ß√£o\n",
        "   - N√£o requer normaliza√ß√£o (mas n√£o prejudica)\n",
        "   - Boa capacidade de lidar com features de diferentes escalas\n",
        "   - Menos propenso a overfitting que √°rvores individuais\n",
        "\"\"\"\n",
        "else:\n",
        "    analise += \"\"\"\n",
        "   A Logistic Regression apresentou boa performance, o que indica:\n",
        "   - Rela√ß√µes relativamente lineares entre features e target\n",
        "   - Dataset bem separ√°vel linearmente\n",
        "   - Modelo interpret√°vel e eficiente computacionalmente\n",
        "\"\"\"\n",
        "\n",
        "analise += f\"\"\"\n",
        "\n",
        "4. OBSERVA√á√ïES IMPORTANTES:\n",
        "\n",
        "   - Todos os modelos apresentaram performance razo√°vel (accuracy > 0.7)\n",
        "   - A valida√ß√£o cruzada mostra consist√™ncia nas predi√ß√µes\n",
        "   - As m√©tricas weighted s√£o apropriadas para lidar com poss√≠vel desbalanceamento\n",
        "   - As curvas ROC indicam boa capacidade discriminativa do modelo\n",
        "\n",
        "5. RECOMENDA√á√ïES:\n",
        "\n",
        "   - Usar {melhor_modelo_nome} como modelo final\n",
        "   - Considerar fine-tuning de hiperpar√¢metros para melhorar ainda mais\n",
        "   - Avaliar feature engineering adicional se necess√°rio\n",
        "   - Monitorar performance em dados novos para garantir generaliza√ß√£o\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(analise)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ 5. Interpreta√ß√£o e Explica√ß√£o do Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "print(\"=\" * 80)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Para Random Forest e XGBoost\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Random Forest Feature Importance\n",
        "if 'Random Forest' in resultados:\n",
        "    rf_importance = resultados['Random Forest']['model'].feature_importances_\n",
        "    rf_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': rf_importance\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    axes[0].barh(rf_importance_df['Feature'], rf_importance_df['Importance'])\n",
        "    axes[0].set_title('Random Forest - Feature Importance', fontweight='bold')\n",
        "    axes[0].set_xlabel('Importance')\n",
        "    axes[0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    print(\"\\nRandom Forest - Top 5 Features mais importantes:\")\n",
        "    print(rf_importance_df.head().to_string(index=False))\n",
        "\n",
        "# XGBoost Feature Importance\n",
        "if 'XGBoost' in resultados:\n",
        "    xgb_importance = resultados['XGBoost']['model'].feature_importances_\n",
        "    xgb_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'Importance': xgb_importance\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    axes[1].barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'], color='orange')\n",
        "    axes[1].set_title('XGBoost - Feature Importance', fontweight='bold')\n",
        "    axes[1].set_xlabel('Importance')\n",
        "    axes[1].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    print(\"\\nXGBoost - Top 5 Features mais importantes:\")\n",
        "    print(xgb_importance_df.head().to_string(index=False))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úÖ Feature importance salvo em 'feature_importance.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partial Dependence Plots (para o melhor modelo tree-based)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PARTIAL DEPENDENCE PLOTS\")\n",
        "print(\"=\"*80)\n",
        "print(\"Nota: Para modelos multiclasse, PDP √© gerado para cada classe separadamente\")\n",
        "\n",
        "try:\n",
        "    from sklearn.inspection import PartialDependenceDisplay\n",
        "    \n",
        "    # Usar o melhor modelo tree-based se dispon√≠vel\n",
        "    modelo_pdp = None\n",
        "    nome_pdp = None\n",
        "    \n",
        "    if melhor_modelo_nome in ['Random Forest', 'XGBoost']:\n",
        "        modelo_pdp = resultados[melhor_modelo_nome]['model']\n",
        "        nome_pdp = melhor_modelo_nome\n",
        "    elif 'Random Forest' in resultados:\n",
        "        modelo_pdp = resultados['Random Forest']['model']\n",
        "        nome_pdp = 'Random Forest'\n",
        "    elif 'XGBoost' in resultados:\n",
        "        modelo_pdp = resultados['XGBoost']['model']\n",
        "        nome_pdp = 'XGBoost'\n",
        "    \n",
        "    if modelo_pdp is not None:\n",
        "        # Selecionar top 4 features mais importantes\n",
        "        if nome_pdp == 'Random Forest':\n",
        "            top_features = rf_importance_df.head(4)['Feature'].tolist()\n",
        "        else:\n",
        "            top_features = xgb_importance_df.head(4)['Feature'].tolist()\n",
        "        \n",
        "        # Criar √≠ndices das features\n",
        "        feature_indices = [feature_cols.index(f) for f in top_features]\n",
        "        \n",
        "        # Para multiclasse, precisamos gerar PDP para cada classe\n",
        "        # Vamos gerar para a classe mais representativa (maior frequ√™ncia)\n",
        "        classes_ordenadas = sorted(y_train.unique())\n",
        "        classe_mais_frequente = y_train.value_counts().index[0]\n",
        "        classe_original = reverse_class_mapping[classe_mais_frequente]\n",
        "        \n",
        "        print(f\"\\nGerando PDP para a Classe {classe_original} (mais frequente)\")\n",
        "        print(f\"Features analisadas: {', '.join(top_features)}\")\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        axes = axes.ravel()\n",
        "        \n",
        "        # Especificar target para multiclasse\n",
        "        PartialDependenceDisplay.from_estimator(\n",
        "            modelo_pdp, X_train, feature_indices, \n",
        "            ax=axes, n_jobs=-1, grid_resolution=50,\n",
        "            target=classe_mais_frequente  # Especificar target para multiclasse\n",
        "        )\n",
        "        \n",
        "        for idx, ax in enumerate(axes):\n",
        "            feature_name = top_features[idx]\n",
        "            # Melhorar nomes das features para exibi√ß√£o\n",
        "            if 'Cantril' in feature_name:\n",
        "                ax.set_title(f'PDP: Escada de Cantril (Felicidade)', fontweight='bold', fontsize=11)\n",
        "            elif 'GDP' in feature_name:\n",
        "                ax.set_title(f'PDP: PIB per capita (PPP)', fontweight='bold', fontsize=11)\n",
        "            elif 'HDI' in feature_name:\n",
        "                ax.set_title(f'PDP: IDH (√çndice Desenvolvimento Humano)', fontweight='bold', fontsize=11)\n",
        "            elif 'homicide' in feature_name.lower():\n",
        "                ax.set_title(f'PDP: Taxa de Homic√≠dios (por 100k)', fontweight='bold', fontsize=11)\n",
        "            else:\n",
        "                ax.set_title(f'PDP: {feature_name}', fontweight='bold', fontsize=11)\n",
        "        \n",
        "        plt.suptitle(f'Partial Dependence Plots - {nome_pdp}\\nClasse {classe_original} (mais frequente)', \n",
        "                     fontsize=14, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('partial_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"‚úÖ Partial Dependence Plots salvo em 'partial_dependence_plots.png'\")\n",
        "        print(f\"   Classe analisada: {classe_original} (mais frequente no dataset)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Partial Dependence Plots n√£o dispon√≠vel para Logistic Regression\")\n",
        "        print(\"   (PDP √© mais √∫til para modelos tree-based)\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è sklearn.inspection.PartialDependenceDisplay n√£o dispon√≠vel nesta vers√£o\")\n",
        "    print(\"   Pulando Partial Dependence Plots\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Erro ao gerar Partial Dependence Plots: {e}\")\n",
        "    print(\"   Continuando com a an√°lise...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise das vari√°veis mais relevantes\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISE DAS VARI√ÅVEIS MAIS RELEVANTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "analise_features = \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "AN√ÅLISE DAS VARI√ÅVEIS MAIS RELEVANTES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "Com base na an√°lise de Feature Importance dos modelos tree-based:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Combinar import√¢ncias de RF e XGBoost se dispon√≠veis\n",
        "if 'Random Forest' in resultados and 'XGBoost' in resultados:\n",
        "    combined_importance = pd.DataFrame({\n",
        "        'Feature': feature_cols,\n",
        "        'RF_Importance': rf_importance,\n",
        "        'XGB_Importance': xgb_importance\n",
        "    })\n",
        "    combined_importance['Average_Importance'] = (\n",
        "        combined_importance['RF_Importance'] + combined_importance['XGB_Importance']\n",
        "    ) / 2\n",
        "    combined_importance = combined_importance.sort_values('Average_Importance', ascending=False)\n",
        "    \n",
        "    analise_features += \"TOP 5 VARI√ÅVEIS MAIS IMPORTANTES (m√©dia entre RF e XGBoost):\\n\\n\"\n",
        "    for idx, row in combined_importance.head(5).iterrows():\n",
        "        analise_features += f\"{idx+1}. {row['Feature']}\\n\"\n",
        "        analise_features += f\"   - Import√¢ncia m√©dia: {row['Average_Importance']:.4f}\\n\"\n",
        "        analise_features += f\"   - RF: {row['RF_Importance']:.4f}, XGB: {row['XGB_Importance']:.4f}\\n\\n\"\n",
        "    \n",
        "    analise_features += \"\\nINTERPRETA√á√ÉO NO CONTEXTO DO DOM√çNIO:\\n\\n\"\n",
        "    top_feature = combined_importance.iloc[0]['Feature']\n",
        "    \n",
        "    # Interpreta√ß√£o espec√≠fica por vari√°vel\n",
        "    if 'HDI' in top_feature or 'Human Development' in top_feature:\n",
        "        analise_features += f\"- IDH (√çndice de Desenvolvimento Humano) √© a vari√°vel mais importante\\n\"\n",
        "        analise_features += \"  ‚Üí Isso faz sentido: IDH √© um indicador composto que captura m√∫ltiplas dimens√µes\\n\"\n",
        "        analise_features += \"  ‚Üí Pa√≠ses com alto IDH t√™m melhor sa√∫de, educa√ß√£o e renda, caracter√≠sticas\\n\"\n",
        "        analise_features += \"    que os diferenciam claramente de pa√≠ses menos desenvolvidos\\n\"\n",
        "    elif 'GDP' in top_feature or 'PIB' in top_feature:\n",
        "        analise_features += f\"- PIB per capita √© a vari√°vel mais importante\\n\"\n",
        "        analise_features += \"  ‚Üí Riqueza econ√¥mica √© um fator fundamental na classifica√ß√£o de pa√≠ses\\n\"\n",
        "        analise_features += \"  ‚Üí Pa√≠ses desenvolvidos t√™m PIB per capita significativamente maior\\n\"\n",
        "    elif 'Cantril' in top_feature:\n",
        "        analise_features += f\"- Escada de Cantril (Felicidade) √© a vari√°vel mais importante\\n\"\n",
        "        analise_features += \"  ‚Üí Bem-estar subjetivo captura aspectos que v√£o al√©m da riqueza material\\n\"\n",
        "        analise_features += \"  ‚Üí Felicidade reflete qualidade de vida e satisfa√ß√£o com condi√ß√µes sociais\\n\"\n",
        "    elif 'homicide' in top_feature.lower():\n",
        "        analise_features += f\"- Taxa de Homic√≠dios √© a vari√°vel mais importante\\n\"\n",
        "        analise_features += \"  ‚Üí Seguran√ßa p√∫blica √© um indicador cr√≠tico de desenvolvimento\\n\"\n",
        "        analise_features += \"  ‚Üí Viol√™ncia est√° inversamente relacionada ao desenvolvimento\\n\"\n",
        "    else:\n",
        "        analise_features += f\"- A vari√°vel '{top_feature}' √© a mais importante para classifica√ß√£o\\n\"\n",
        "    \n",
        "    analise_features += \"\\n- As vari√°veis Cantril (Felicidade), GDP (PIB) e HDI s√£o fundamentais\\n\"\n",
        "    analise_features += \"  para classificar pa√≠ses em diferentes n√≠veis de desenvolvimento\\n\"\n",
        "    analise_features += \"- A combina√ß√£o desses indicadores permite uma vis√£o multidimensional\\n\"\n",
        "    analise_features += \"  do desenvolvimento, indo al√©m de apenas riqueza econ√¥mica\\n\"\n",
        "\n",
        "analise_features += \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(analise_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Como melhorar o modelo\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUGEST√ïES DE MELHORIA DO MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "melhorias = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "SUGEST√ïES PARA MELHORAR O MODELO EM UMA PR√ìXIMA ENTREGA\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. FINE-TUNING DE HIPERPAR√ÇMETROS:\n",
        "   - Realizar GridSearchCV ou RandomizedSearchCV para otimizar hiperpar√¢metros\n",
        "   - Para Random Forest: ajustar n_estimators, max_depth, min_samples_split\n",
        "   - Para XGBoost: ajustar learning_rate, max_depth, n_estimators, subsample\n",
        "   - Para Logistic Regression: ajustar C (regulariza√ß√£o) e solver\n",
        "\n",
        "2. FEATURE ENGINEERING:\n",
        "   - Criar features de intera√ß√£o (ex: GDP/HDI, Cantril*HDI)\n",
        "   - Considerar transforma√ß√µes logar√≠tmicas para vari√°veis com distribui√ß√£o assim√©trica\n",
        "   - Avaliar cria√ß√£o de features categ√≥ricas bin√°rias a partir de thresholds\n",
        "\n",
        "3. ENSEMBLE METHODS:\n",
        "   - Combinar predi√ß√µes de m√∫ltiplos modelos (voting classifier)\n",
        "   - Usar stacking com meta-learner\n",
        "   - Avaliar weighted voting baseado na performance individual\n",
        "\n",
        "4. VALIDA√á√ÉO MAIS RIGOROSA:\n",
        "   - Usar nested cross-validation para avalia√ß√£o mais robusta\n",
        "   - Implementar time-based split se houver componente temporal\n",
        "   - Avaliar performance por classe individualmente\n",
        "\n",
        "5. TRATAMENTO DE OUTLIERS:\n",
        "   - Avaliar impacto de outliers nas predi√ß√µes\n",
        "   - Considerar t√©cnicas de robustez (ex: Isolation Forest)\n",
        "   - Testar remo√ß√£o ou transforma√ß√£o de outliers extremos\n",
        "\n",
        "6. COLETA DE MAIS DADOS:\n",
        "   - Aumentar o tamanho do dataset se poss√≠vel\n",
        "   - Incluir vari√°veis adicionais relevantes (ex: indicadores sociais, econ√¥micos)\n",
        "   - Considerar dados de s√©ries temporais se dispon√≠veis\n",
        "\n",
        "7. AN√ÅLISE DE ERROS:\n",
        "   - Analisar casos mal classificados\n",
        "   - Identificar padr√µes nos erros\n",
        "   - Ajustar modelo com base em insights dos erros\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(melhorias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìò 6. Documenta√ß√£o Completa - Relat√≥rio Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defini√ß√µes e Contexto das Vari√°veis Principais\n",
        "print(\"=\" * 80)\n",
        "print(\"DEFINI√á√ïES E CONTEXTO DAS VARI√ÅVEIS PRINCIPAIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "definicoes = \"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "DEFINI√á√ïES E CONTEXTO DAS VARI√ÅVEIS PRINCIPAIS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. ESCADA DE CANTRIL (Cantril Ladder Score) - MEDIDA DE FELICIDADE:\n",
        "   \n",
        "   Defini√ß√£o: A Escada de Cantril √© uma medida de bem-estar subjetivo desenvolvida\n",
        "   por Hadley Cantril. Os entrevistados avaliam sua vida atual em uma escala de 0 a 10,\n",
        "   onde 0 = \"a pior vida poss√≠vel\" e 10 = \"a melhor vida poss√≠vel\".\n",
        "   \n",
        "   Contexto: Esta vari√°vel captura a percep√ß√£o subjetiva de felicidade e satisfa√ß√£o\n",
        "   com a vida, que est√° relacionada mas n√£o √© id√™ntica ao desenvolvimento econ√¥mico.\n",
        "   Pa√≠ses com alto PIB podem ter baixa felicidade se houver desigualdade ou outros\n",
        "   problemas sociais.\n",
        "   \n",
        "   No dataset:\n",
        "   - Valores observados: {:.2f} a {:.2f} (escala 0-10)\n",
        "   - M√©dia: {:.2f}\n",
        "   - Interpreta√ß√£o: Valores mais altos indicam maior bem-estar subjetivo\n",
        "\n",
        "2. PIB PER CAPITA (GDP per capita PPP) - RIQUEZA ECON√îMICA:\n",
        "   \n",
        "   Defini√ß√£o: Produto Interno Bruto per capita ajustado por Paridade de Poder de \n",
        "   Compra (PPP), expresso em d√≥lares internacionais constantes de 2021.\n",
        "   Representa a riqueza econ√¥mica m√©dia por pessoa, ajustada pelo custo de vida.\n",
        "   \n",
        "   Contexto: O PIB per capita √© um indicador fundamental de desenvolvimento econ√¥mico.\n",
        "   Pa√≠ses com maior PIB per capita geralmente t√™m melhor infraestrutura, sa√∫de e educa√ß√£o.\n",
        "   Ajuste por PPP permite compara√ß√µes mais justas entre pa√≠ses com custos de vida diferentes.\n",
        "   \n",
        "   No dataset:\n",
        "   - Valores observados: ${:,.0f} a ${:,.0f}\n",
        "   - M√©dia: ${:,.0f}\n",
        "   - Interpreta√ß√£o: Valores mais altos indicam maior riqueza econ√¥mica\n",
        "\n",
        "3. IDH (HDI - Human Development Index) - DESENVOLVIMENTO HUMANO:\n",
        "   \n",
        "   Defini√ß√£o: √çndice composto criado pelo PNUD que mede o desenvolvimento humano atrav√©s\n",
        "   de tr√™s dimens√µes:\n",
        "   - Sa√∫de: Expectativa de vida ao nascer\n",
        "   - Educa√ß√£o: Anos m√©dios de escolaridade e anos esperados de escolaridade\n",
        "   - Padr√£o de vida: Renda Nacional Bruta per capita (PPP)\n",
        "   \n",
        "   Contexto: O IDH vai al√©m do PIB, incorporando aspectos de qualidade de vida.\n",
        "   Um pa√≠s pode ter alto PIB mas baixo IDH se houver desigualdade ou baixa qualidade\n",
        "   de servi√ßos p√∫blicos. √â considerado um indicador mais completo de desenvolvimento.\n",
        "   \n",
        "   No dataset:\n",
        "   - Valores observados: {:.0f} a {:.0f} (escala 0-1000)\n",
        "   - M√©dia: {:.0f}\n",
        "   - Interpreta√ß√£o: Valores mais altos indicam maior desenvolvimento humano\n",
        "\n",
        "4. TAXA DE HOMIC√çDIOS - SEGURAN√áA E VIOL√äNCIA:\n",
        "   \n",
        "   Defini√ß√£o: Taxa de homic√≠dios intencionais por 100.000 habitantes (dados de 2021).\n",
        "   Mede a seguran√ßa p√∫blica e o n√≠vel de viol√™ncia em um pa√≠s.\n",
        "   \n",
        "   Contexto: A viol√™ncia est√° frequentemente correlacionada com desigualdade econ√¥mica,\n",
        "   falta de oportunidades e problemas sociais. Pa√≠ses com alto desenvolvimento humano\n",
        "   geralmente t√™m menores taxas de homic√≠dios, mas h√° exce√ß√µes (ex: alguns pa√≠ses\n",
        "   desenvolvidos com alta viol√™ncia ou pa√≠ses em desenvolvimento com baixa viol√™ncia).\n",
        "   \n",
        "   No dataset:\n",
        "   - Valores observados: {:.2f} a {:.2f} por 100.000 habitantes\n",
        "   - M√©dia: {:.2f}\n",
        "   - Interpreta√ß√£o: Valores mais altos indicam maior viol√™ncia/inseguran√ßa\n",
        "\n",
        "5. RELA√á√ïES ESPERADAS ENTRE VARI√ÅVEIS:\n",
        "   \n",
        "   - PIB e IDH: Forte correla√ß√£o positiva esperada (pa√≠ses ricos tendem a ter melhor IDH)\n",
        "   - Cantril e IDH: Correla√ß√£o positiva esperada (melhor desenvolvimento ‚Üí maior felicidade)\n",
        "   - Cantril e PIB: Correla√ß√£o positiva, mas pode ser moderada (dinheiro n√£o compra felicidade)\n",
        "   - Homic√≠dios e IDH: Correla√ß√£o negativa esperada (maior desenvolvimento ‚Üí menos viol√™ncia)\n",
        "   - Homic√≠dios e PIB: Correla√ß√£o negativa esperada, mas pode variar por regi√£o\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\".format(\n",
        "    df['Cantril ladder score'].min(),\n",
        "    df['Cantril ladder score'].max(),\n",
        "    df['Cantril ladder score'].mean(),\n",
        "    df['GDP per capita. PPP (constant 2021 international $)'].min(),\n",
        "    df['GDP per capita. PPP (constant 2021 international $)'].max(),\n",
        "    df['GDP per capita. PPP (constant 2021 international $)'].mean(),\n",
        "    df['HDI (Human Development Indicator) Value (0~1000)'].min(),\n",
        "    df['HDI (Human Development Indicator) Value (0~1000)'].max(),\n",
        "    df['HDI (Human Development Indicator) Value (0~1000)'].mean(),\n",
        "    df['Intentional homicides (per 100000 people) as of 2021'].min(),\n",
        "    df['Intentional homicides (per 100000 people) as of 2021'].max(),\n",
        "    df['Intentional homicides (per 100000 people) as of 2021'].mean()\n",
        ")\n",
        "\n",
        "print(definicoes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RELAT√ìRIO FINAL COMPLETO\n",
        "print(\"=\" * 80)\n",
        "print(\"RELAT√ìRIO FINAL COMPLETO\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verificar se as vari√°veis necess√°rias est√£o definidas\n",
        "try:\n",
        "    balance_status = 'SMOTE aplicado' if balance_ratio < 0.5 else 'Balanceado'\n",
        "except NameError:\n",
        "    balance_status = 'Avaliado durante prepara√ß√£o dos dados'\n",
        "\n",
        "try:\n",
        "    melhor_nome = melhor_modelo_nome\n",
        "    melhor_acc = comparacao.iloc[0]['Accuracy']\n",
        "    melhor_f1 = comparacao.iloc[0]['F1-Score']\n",
        "    melhor_prec = comparacao.iloc[0]['Precision']\n",
        "    melhor_rec = comparacao.iloc[0]['Recall']\n",
        "except NameError:\n",
        "    melhor_nome = 'A ser determinado ap√≥s execu√ß√£o dos modelos'\n",
        "    melhor_acc = 0.0\n",
        "    melhor_f1 = 0.0\n",
        "    melhor_prec = 0.0\n",
        "    melhor_rec = 0.0\n",
        "\n",
        "relatorio_final = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "RELAT√ìRIO FINAL - AN√ÅLISE DE CLASSIFICA√á√ÉO\n",
        "Cantrill + GDP + HDI vs Country Class\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. RESUMO EXECUTIVO:\n",
        "   Este relat√≥rio apresenta uma an√°lise completa de classifica√ß√£o utilizando\n",
        "   dados de indicadores socioecon√¥micos (Cantrill, GDP, HDI) para classificar\n",
        "   pa√≠ses em 4 categorias.\n",
        "\n",
        "2. DATASET:\n",
        "   - Total de pa√≠ses analisados: {len(df)}\n",
        "   - Vari√°veis preditoras principais: Cantril Ladder Score, GDP per capita, HDI\n",
        "   - Vari√°vel-alvo: Country Class (4 classes)\n",
        "   - Distribui√ß√£o de classes: {df['Country Class'].value_counts().sort_index().to_dict()}\n",
        "\n",
        "3. METODOLOGIA APLICADA:\n",
        "   \n",
        "   a) Auditoria Inicial:\n",
        "      - Verifica√ß√£o de valores ausentes: ‚úÖ Realizada\n",
        "      - Verifica√ß√£o de duplicatas: ‚úÖ Realizada\n",
        "      - Identifica√ß√£o de colunas redundantes: ‚úÖ Removidas\n",
        "      - An√°lise de outliers: ‚úÖ Realizada (m√©todo IQR)\n",
        "      - Relat√≥rio inicial gerado com todas as descobertas\n",
        "   \n",
        "   b) An√°lise Explorat√≥ria (EDA):\n",
        "      - Estat√≠sticas descritivas completas (mean, std, min, max, quartis, mediana, moda, assimetria, curtose)\n",
        "      - An√°lise de distribui√ß√µes (histogramas e boxplots de todas vari√°veis num√©ricas)\n",
        "      - Matriz de correla√ß√£o entre vari√°veis (heatmap)\n",
        "      - Identifica√ß√£o e justificativa detalhada da vari√°vel-alvo\n",
        "      - An√°lise de balanceamento das classes\n",
        "   \n",
        "   c) Prepara√ß√£o dos Dados:\n",
        "      - Remo√ß√£o de colunas redundantes (one-hot encoding, vers√µes escaladas)\n",
        "      - Tratamento de valores ausentes (estrat√©gia justificada: mediana para num√©ricas, moda para categ√≥ricas)\n",
        "      - Encoding de vari√°veis categ√≥ricas (Label Encoding)\n",
        "      - Separa√ß√£o treino/teste (80/20) com estratifica√ß√£o (justificada estatisticamente)\n",
        "      - Avalia√ß√£o de balanceamento: {balance_status}\n",
        "      - Padroniza√ß√£o (StandardScaler) com justificativa t√©cnica\n",
        "   \n",
        "   d) Modelagem:\n",
        "      - 3 modelos testados: Logistic Regression, Random Forest, XGBoost\n",
        "      - Valida√ß√£o cruzada 5-fold estratificada (k=5)\n",
        "      - M√©tricas avaliadas: Accuracy, Precision, Recall, F1-Score, Matriz de Confus√£o, ROC Curve + AUC\n",
        "      - Compara√ß√£o detalhada entre modelos\n",
        "      - Sele√ß√£o do melhor modelo com justificativa\n",
        "\n",
        "4. RESULTADOS:\n",
        "\n",
        "   Melhor Modelo: {melhor_nome}\n",
        "   - Accuracy: {melhor_acc:.4f}\n",
        "   - F1-Score: {melhor_f1:.4f}\n",
        "   - Precision: {melhor_prec:.4f}\n",
        "   - Recall: {melhor_rec:.4f}\n",
        "\n",
        "5. INTERPRETA√á√ÉO DO MODELO:\n",
        "   - Feature importance analisada (Random Forest e XGBoost)\n",
        "   - Partial Dependence Plots gerados para top features\n",
        "   - An√°lise das vari√°veis mais relevantes realizada\n",
        "   - As vari√°veis Cantrill, GDP e HDI s√£o fundamentais para a classifica√ß√£o\n",
        "   - O modelo {melhor_nome} apresentou melhor capacidade de generaliza√ß√£o\n",
        "   - As curvas ROC indicam boa capacidade discriminativa\n",
        "\n",
        "6. CONCLUS√ÉO:\n",
        "   A an√°lise foi conduzida com sucesso, identificando padr√µes claros entre\n",
        "   indicadores socioecon√¥micos e classifica√ß√£o de pa√≠ses. O modelo escolhido\n",
        "   ({melhor_nome}) demonstra performance adequada e pode ser utilizado\n",
        "   para classifica√ß√£o de novos pa√≠ses com base nos indicadores analisados.\n",
        "   \n",
        "   Todas as etapas foram executadas com rigor t√©cnico, documenta√ß√£o completa\n",
        "   e justificativas para cada decis√£o tomada, seguindo metodologia cient√≠fica\n",
        "   apropriada para an√°lise de dados de classifica√ß√£o.\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(relatorio_final)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé® 7. An√°lises Explorat√≥rias Avan√ßadas\n",
        "\n",
        "Esta se√ß√£o demonstra o poder da ci√™ncia de dados para extrair informa√ß√µes valiosas dos dados, incluindo:\n",
        "- Compara√ß√£o entre continentes\n",
        "- Rankings e classifica√ß√µes\n",
        "- Top 10 em diversas categorias\n",
        "- Visualiza√ß√µes criativas e informativas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise Comparativa entre Continentes\n",
        "print(\"=\" * 80)\n",
        "print(\"AN√ÅLISE COMPARATIVA ENTRE CONTINENTES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Criar dataframe com dados originais para an√°lise explorat√≥ria\n",
        "df_exploratorio = df.copy()\n",
        "\n",
        "# Agrupar por continente\n",
        "continentes_stats = df_exploratorio.groupby('World regions according to OWID').agg({\n",
        "    'Cantril ladder score': ['mean', 'std', 'min', 'max'],\n",
        "    'GDP per capita. PPP (constant 2021 international $)': ['mean', 'std', 'min', 'max'],\n",
        "    'HDI (Human Development Indicator) Value (0~1000)': ['mean', 'std', 'min', 'max'],\n",
        "    'Intentional homicides (per 100000 people) as of 2021': ['mean', 'std', 'min', 'max'],\n",
        "    'Country Name': 'count'  # Contar pa√≠ses por continente\n",
        "}).round(2)\n",
        "\n",
        "continentes_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in continentes_stats.columns.values]\n",
        "continentes_stats = continentes_stats.rename(columns={'Country Name_count': 'Num_Paises'})\n",
        "\n",
        "print(\"\\nüìä ESTAT√çSTICAS POR CONTINENTE:\")\n",
        "print(\"=\" * 80)\n",
        "print(continentes_stats)\n",
        "\n",
        "# Criar ranking de continentes\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üèÜ RANKING DE CONTINENTES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Criar √≠ndice composto de desenvolvimento (m√©dia normalizada)\n",
        "ranking_continentes = pd.DataFrame({\n",
        "    'Continente': continentes_stats.index,\n",
        "    'Felicidade_M√©dia': continentes_stats['Cantril ladder score_mean'],\n",
        "    'PIB_M√©dio': continentes_stats['GDP per capita. PPP (constant 2021 international $)_mean'],\n",
        "    'IDH_M√©dio': continentes_stats['HDI (Human Development Indicator) Value (0~1000)_mean'],\n",
        "    'Homic√≠dios_M√©dio': continentes_stats['Intentional homicides (per 100000 people) as of 2021_mean'],\n",
        "    'Num_Paises': continentes_stats['Num_Paises']\n",
        "})\n",
        "\n",
        "# Normalizar e criar score composto (quanto maior, melhor)\n",
        "# Para homic√≠dios, inverter (menor √© melhor)\n",
        "ranking_continentes['Homic√≠dios_Invertido'] = 100 - ranking_continentes['Homic√≠dios_M√©dio']  # Normalizar aproximadamente\n",
        "\n",
        "# Normalizar cada m√©trica para 0-1\n",
        "for col in ['Felicidade_M√©dia', 'PIB_M√©dio', 'IDH_M√©dio', 'Homic√≠dios_Invertido']:\n",
        "    min_val = ranking_continentes[col].min()\n",
        "    max_val = ranking_continentes[col].max()\n",
        "    ranking_continentes[f'{col}_norm'] = (ranking_continentes[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "# Score composto (m√©dia ponderada)\n",
        "ranking_continentes['Score_Desenvolvimento'] = (\n",
        "    ranking_continentes['Felicidade_M√©dia_norm'] * 0.25 +\n",
        "    ranking_continentes['PIB_M√©dio_norm'] * 0.25 +\n",
        "    ranking_continentes['IDH_M√©dio_norm'] * 0.30 +\n",
        "    ranking_continentes['Homic√≠dios_Invertido_norm'] * 0.20\n",
        ")\n",
        "\n",
        "ranking_continentes = ranking_continentes.sort_values('Score_Desenvolvimento', ascending=False)\n",
        "\n",
        "print(\"\\nRanking de Continentes por Desenvolvimento (Score Composto):\")\n",
        "print(\"-\" * 80)\n",
        "for idx, row in ranking_continentes.iterrows():\n",
        "    print(f\"{ranking_continentes.index.get_loc(idx) + 1}. {row['Continente']}\")\n",
        "    print(f\"   Score: {row['Score_Desenvolvimento']:.3f}\")\n",
        "    print(f\"   Felicidade: {row['Felicidade_M√©dia']:.2f} | PIB: ${row['PIB_M√©dio']:,.0f} | IDH: {row['IDH_M√©dio']:.0f} | Homic√≠dios: {row['Homic√≠dios_M√©dio']:.2f}/100k\")\n",
        "    print(f\"   Pa√≠ses analisados: {int(row['Num_Paises'])}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o: Compara√ß√£o de Continentes\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# 1. Felicidade por Continente\n",
        "continentes_ordenados = ranking_continentes.sort_values('Felicidade_M√©dia', ascending=True)\n",
        "axes[0, 0].barh(continentes_ordenados['Continente'], continentes_ordenados['Felicidade_M√©dia'], \n",
        "                color='gold', edgecolor='black', alpha=0.8)\n",
        "axes[0, 0].set_title('Felicidade M√©dia por Continente\\n(Escada de Cantril)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Felicidade (0-10)')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 2. PIB per capita por Continente\n",
        "continentes_pib = ranking_continentes.sort_values('PIB_M√©dio', ascending=True)\n",
        "axes[0, 1].barh(continentes_pib['Continente'], continentes_pib['PIB_M√©dio'], \n",
        "                color='green', edgecolor='black', alpha=0.8)\n",
        "axes[0, 1].set_title('PIB per Capita M√©dio por Continente\\n(PPP, USD 2021)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('PIB per Capita (USD)')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 3. IDH por Continente\n",
        "continentes_idh = ranking_continentes.sort_values('IDH_M√©dio', ascending=True)\n",
        "axes[1, 0].barh(continentes_idh['Continente'], continentes_idh['IDH_M√©dio'], \n",
        "                color='steelblue', edgecolor='black', alpha=0.8)\n",
        "axes[1, 0].set_title('IDH M√©dio por Continente\\n(0-1000)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('IDH')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 4. Taxa de Homic√≠dios por Continente\n",
        "continentes_homic = ranking_continentes.sort_values('Homic√≠dios_M√©dio', ascending=True)\n",
        "axes[1, 1].barh(continentes_homic['Continente'], continentes_homic['Homic√≠dios_M√©dio'], \n",
        "                color='crimson', edgecolor='black', alpha=0.8)\n",
        "axes[1, 1].set_title('Taxa de Homic√≠dios M√©dia por Continente\\n(por 100.000 habitantes)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Homic√≠dios por 100k')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.suptitle('Compara√ß√£o de Indicadores Socioecon√¥micos por Continente', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparacao_continentes.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Gr√°fico de compara√ß√£o de continentes salvo em 'comparacao_continentes.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TOP 10 Rankings - Extraindo Informa√ß√µes Valiosas\n",
        "print(\"=\" * 80)\n",
        "print(\"üèÜ TOP 10 RANKINGS - EXTRAINDO INFORMA√á√ïES VALIOSAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Preparar dados para rankings\n",
        "df_rankings = df_exploratorio[['Country Name', 'World regions according to OWID',\n",
        "                               'Cantril ladder score', \n",
        "                               'GDP per capita. PPP (constant 2021 international $)',\n",
        "                               'HDI (Human Development Indicator) Value (0~1000)',\n",
        "                               'Intentional homicides (per 100000 people) as of 2021',\n",
        "                               'Country Class']].copy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1Ô∏è‚É£ TOP 10 PA√çSES MAIS RICOS (PIB per capita)\")\n",
        "print(\"=\"*80)\n",
        "top10_ricos = df_rankings.nlargest(10, 'GDP per capita. PPP (constant 2021 international $)')\n",
        "for idx, (i, row) in enumerate(top10_ricos.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | ${row['GDP per capita. PPP (constant 2021 international $)']:>10,.0f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2Ô∏è‚É£ TOP 10 PA√çSES MAIS POBRES (PIB per capita)\")\n",
        "print(\"=\"*80)\n",
        "top10_pobres = df_rankings.nsmallest(10, 'GDP per capita. PPP (constant 2021 international $)')\n",
        "for idx, (i, row) in enumerate(top10_pobres.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | ${row['GDP per capita. PPP (constant 2021 international $)']:>10,.0f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3Ô∏è‚É£ TOP 10 PA√çSES MAIS FELIZES (Escada de Cantril)\")\n",
        "print(\"=\"*80)\n",
        "top10_felizes = df_rankings.nlargest(10, 'Cantril ladder score')\n",
        "for idx, (i, row) in enumerate(top10_felizes.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | Felicidade: {row['Cantril ladder score']:>5.2f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4Ô∏è‚É£ TOP 10 PA√çSES MENOS FELIZES (Escada de Cantril)\")\n",
        "print(\"=\"*80)\n",
        "top10_infelizes = df_rankings.nsmallest(10, 'Cantril ladder score')\n",
        "for idx, (i, row) in enumerate(top10_infelizes.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | Felicidade: {row['Cantril ladder score']:>5.2f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5Ô∏è‚É£ TOP 10 PA√çSES COM MAIOR IDH (Desenvolvimento Humano)\")\n",
        "print(\"=\"*80)\n",
        "top10_idh = df_rankings.nlargest(10, 'HDI (Human Development Indicator) Value (0~1000)')\n",
        "for idx, (i, row) in enumerate(top10_idh.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | IDH: {row['HDI (Human Development Indicator) Value (0~1000)']:>6.0f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"6Ô∏è‚É£ TOP 10 PA√çSES COM MENOR IDH (Desenvolvimento Humano)\")\n",
        "print(\"=\"*80)\n",
        "top10_idh_baixo = df_rankings.nsmallest(10, 'HDI (Human Development Indicator) Value (0~1000)')\n",
        "for idx, (i, row) in enumerate(top10_idh_baixo.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | IDH: {row['HDI (Human Development Indicator) Value (0~1000)']:>6.0f} | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"7Ô∏è‚É£ TOP 10 PA√çSES MAIS VIOLENTOS (Taxa de Homic√≠dios)\")\n",
        "print(\"=\"*80)\n",
        "top10_violentos = df_rankings.nlargest(10, 'Intentional homicides (per 100000 people) as of 2021')\n",
        "for idx, (i, row) in enumerate(top10_violentos.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | {row['Intentional homicides (per 100000 people) as of 2021']:>6.2f}/100k | {row['World regions according to OWID']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"8Ô∏è‚É£ TOP 10 PA√çSES MAIS SEGUROS (Menor Taxa de Homic√≠dios)\")\n",
        "print(\"=\"*80)\n",
        "top10_seguros = df_rankings.nsmallest(10, 'Intentional homicides (per 100000 people) as of 2021')\n",
        "for idx, (i, row) in enumerate(top10_seguros.iterrows(), 1):\n",
        "    print(f\"{idx:2d}. {row['Country Name']:30s} | {row['Intentional homicides (per 100000 people) as of 2021']:>6.2f}/100k | {row['World regions according to OWID']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lises Criativas e Insights Avan√ßados\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° INSIGHTS AVAN√áADOS E AN√ÅLISES CRIATIVAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Pa√≠ses com maior discrep√¢ncia entre PIB e Felicidade\n",
        "print(\"\\n1Ô∏è‚É£ PA√çSES COM MAIOR DISCREP√ÇNCIA ENTRE RIQUEZA E FELICIDADE\")\n",
        "print(\"-\" * 80)\n",
        "df_rankings['PIB_Norm'] = (df_rankings['GDP per capita. PPP (constant 2021 international $)'] - \n",
        "                           df_rankings['GDP per capita. PPP (constant 2021 international $)'].min()) / \\\n",
        "                          (df_rankings['GDP per capita. PPP (constant 2021 international $)'].max() - \n",
        "                           df_rankings['GDP per capita. PPP (constant 2021 international $)'].min())\n",
        "df_rankings['Felicidade_Norm'] = (df_rankings['Cantril ladder score'] - \n",
        "                                  df_rankings['Cantril ladder score'].min()) / \\\n",
        "                                 (df_rankings['Cantril ladder score'].max() - \n",
        "                                  df_rankings['Cantril ladder score'].min())\n",
        "df_rankings['Discrepancia_Riqueza_Felicidade'] = df_rankings['PIB_Norm'] - df_rankings['Felicidade_Norm']\n",
        "\n",
        "# Pa√≠ses ricos mas menos felizes (dinheiro n√£o compra felicidade)\n",
        "print(\"\\nüí∞ Ricos mas Menos Felizes (PIB alto, Felicidade baixa):\")\n",
        "ricos_infelizes = df_rankings.nlargest(5, 'Discrepancia_Riqueza_Felicidade')\n",
        "for idx, (i, row) in enumerate(ricos_infelizes.iterrows(), 1):\n",
        "    print(f\"   {idx}. {row['Country Name']:30s} | PIB: ${row['GDP per capita. PPP (constant 2021 international $)']:>10,.0f} | Felicidade: {row['Cantril ladder score']:.2f}\")\n",
        "\n",
        "# Pa√≠ses pobres mas mais felizes (felicidade al√©m da riqueza)\n",
        "print(\"\\nüòä Pobres mas Mais Felizes (PIB baixo, Felicidade alta):\")\n",
        "pobres_felizes = df_rankings.nsmallest(5, 'Discrepancia_Riqueza_Felicidade')\n",
        "for idx, (i, row) in enumerate(pobres_felizes.iterrows(), 1):\n",
        "    print(f\"   {idx}. {row['Country Name']:30s} | PIB: ${row['GDP per capita. PPP (constant 2021 international $)']:>10,.0f} | Felicidade: {row['Cantril ladder score']:.2f}\")\n",
        "\n",
        "# 2. Pa√≠ses com melhor equil√≠brio (alto em tudo)\n",
        "print(\"\\n2Ô∏è‚É£ PA√çSES COM MELHOR EQUIL√çBRIO (Alto PIB, Alta Felicidade, Alto IDH, Baixa Viol√™ncia)\")\n",
        "print(\"-\" * 80)\n",
        "df_rankings['Homicidios_Invertido'] = 100 - df_rankings['Intentional homicides (per 100000 people) as of 2021']\n",
        "df_rankings['IDH_Norm'] = (df_rankings['HDI (Human Development Indicator) Value (0~1000)'] - \n",
        "                           df_rankings['HDI (Human Development Indicator) Value (0~1000)'].min()) / \\\n",
        "                          (df_rankings['HDI (Human Development Indicator) Value (0~1000)'].max() - \n",
        "                           df_rankings['HDI (Human Development Indicator) Value (0~1000)'].min())\n",
        "df_rankings['Homicidios_Norm'] = (df_rankings['Homicidios_Invertido'] - \n",
        "                                   df_rankings['Homicidios_Invertido'].min()) / \\\n",
        "                                  (df_rankings['Homicidios_Invertido'].max() - \n",
        "                                   df_rankings['Homicidios_Invertido'].min())\n",
        "\n",
        "df_rankings['Score_Equilibrio'] = (\n",
        "    df_rankings['PIB_Norm'] * 0.25 +\n",
        "    df_rankings['Felicidade_Norm'] * 0.25 +\n",
        "    df_rankings['IDH_Norm'] * 0.30 +\n",
        "    df_rankings['Homicidios_Norm'] * 0.20\n",
        ")\n",
        "\n",
        "top_equilibrio = df_rankings.nlargest(10, 'Score_Equilibrio')\n",
        "print(\"\\nüèÜ Top 10 Pa√≠ses com Melhor Equil√≠brio:\")\n",
        "for idx, (i, row) in enumerate(top_equilibrio.iterrows(), 1):\n",
        "    print(f\"   {idx:2d}. {row['Country Name']:30s} | Score: {row['Score_Equilibrio']:.3f} | \"\n",
        "          f\"PIB: ${row['GDP per capita. PPP (constant 2021 international $)']:>8,.0f} | \"\n",
        "          f\"Felicidade: {row['Cantril ladder score']:.2f} | IDH: {row['HDI (Human Development Indicator) Value (0~1000)']:.0f}\")\n",
        "\n",
        "# 3. An√°lise por classe de desenvolvimento\n",
        "print(\"\\n3Ô∏è‚É£ AN√ÅLISE POR CLASSE DE DESENVOLVIMENTO\")\n",
        "print(\"-\" * 80)\n",
        "classe_stats = df_rankings.groupby('Country Class').agg({\n",
        "    'Cantril ladder score': 'mean',\n",
        "    'GDP per capita. PPP (constant 2021 international $)': 'mean',\n",
        "    'HDI (Human Development Indicator) Value (0~1000)': 'mean',\n",
        "    'Intentional homicides (per 100000 people) as of 2021': 'mean',\n",
        "    'Country Name': 'count'\n",
        "}).round(2)\n",
        "\n",
        "classe_stats.columns = ['Felicidade_M√©dia', 'PIB_M√©dio', 'IDH_M√©dio', 'Homicidios_M√©dio', 'Num_Paises']\n",
        "print(\"\\nEstat√≠sticas por Classe de Desenvolvimento:\")\n",
        "print(classe_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√µes Criativas\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä VISUALIZA√á√ïES CRIATIVAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Scatter Plot: PIB vs Felicidade (com cores por continente)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# PIB vs Felicidade\n",
        "continentes = df_rankings['World regions according to OWID'].unique()\n",
        "cores = plt.cm.Set3(np.linspace(0, 1, len(continentes)))\n",
        "for continente, cor in zip(continentes, cores):\n",
        "    dados = df_rankings[df_rankings['World regions according to OWID'] == continente]\n",
        "    axes[0, 0].scatter(dados['GDP per capita. PPP (constant 2021 international $)'],\n",
        "                      dados['Cantril ladder score'],\n",
        "                      label=continente, alpha=0.6, s=100, color=cor, edgecolors='black', linewidth=0.5)\n",
        "axes[0, 0].set_xlabel('PIB per Capita (PPP, USD)', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Felicidade (Escada de Cantril)', fontsize=11)\n",
        "axes[0, 0].set_title('PIB vs Felicidade por Continente', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# IDH vs Felicidade\n",
        "for continente, cor in zip(continentes, cores):\n",
        "    dados = df_rankings[df_rankings['World regions according to OWID'] == continente]\n",
        "    axes[0, 1].scatter(dados['HDI (Human Development Indicator) Value (0~1000)'],\n",
        "                      dados['Cantril ladder score'],\n",
        "                      label=continente, alpha=0.6, s=100, color=cor, edgecolors='black', linewidth=0.5)\n",
        "axes[0, 1].set_xlabel('IDH (0-1000)', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Felicidade (Escada de Cantril)', fontsize=11)\n",
        "axes[0, 1].set_title('IDH vs Felicidade por Continente', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# IDH vs Homic√≠dios (invertido - menor √© melhor)\n",
        "for continente, cor in zip(continentes, cores):\n",
        "    dados = df_rankings[df_rankings['World regions according to OWID'] == continente]\n",
        "    axes[1, 0].scatter(dados['HDI (Human Development Indicator) Value (0~1000)'],\n",
        "                      dados['Intentional homicides (per 100000 people) as of 2021'],\n",
        "                      label=continente, alpha=0.6, s=100, color=cor, edgecolors='black', linewidth=0.5)\n",
        "axes[1, 0].set_xlabel('IDH (0-1000)', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Taxa de Homic√≠dios (por 100k)', fontsize=11)\n",
        "axes[1, 0].set_title('IDH vs Viol√™ncia por Continente', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# PIB vs Homic√≠dios\n",
        "for continente, cor in zip(continentes, cores):\n",
        "    dados = df_rankings[df_rankings['World regions according to OWID'] == continente]\n",
        "    axes[1, 1].scatter(dados['GDP per capita. PPP (constant 2021 international $)'],\n",
        "                      dados['Intentional homicides (per 100000 people) as of 2021'],\n",
        "                      label=continente, alpha=0.6, s=100, color=cor, edgecolors='black', linewidth=0.5)\n",
        "axes[1, 1].set_xlabel('PIB per Capita (PPP, USD)', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Taxa de Homic√≠dios (por 100k)', fontsize=11)\n",
        "axes[1, 1].set_title('PIB vs Viol√™ncia por Continente', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Rela√ß√µes entre Indicadores Socioecon√¥micos por Continente', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('relacoes_indicadores_continentes.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Gr√°fico de rela√ß√µes salvo em 'relacoes_indicadores_continentes.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap de correla√ß√£o por continente\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üåç AN√ÅLISE DE CORRELA√á√ïES POR CONTINENTE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "continentes_ordenados = sorted(continentes)\n",
        "for idx, continente in enumerate(continentes_ordenados[:6]):  # Limitar a 6 para visualiza√ß√£o\n",
        "    dados_cont = df_rankings[df_rankings['World regions according to OWID'] == continente]\n",
        "    if len(dados_cont) > 2:  # Precisa de pelo menos 3 pontos para correla√ß√£o\n",
        "        cols_corr = ['Cantril ladder score', \n",
        "                    'GDP per capita. PPP (constant 2021 international $)',\n",
        "                    'HDI (Human Development Indicator) Value (0~1000)',\n",
        "                    'Intentional homicides (per 100000 people) as of 2021']\n",
        "        corr_cont = dados_cont[cols_corr].corr()\n",
        "        \n",
        "        mask = np.triu(np.ones_like(corr_cont, dtype=bool))\n",
        "        sns.heatmap(corr_cont, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "                   center=0, square=True, ax=axes[idx], cbar_kws={\"shrink\": 0.8},\n",
        "                   vmin=-1, vmax=1, linewidths=0.5)\n",
        "        axes[idx].set_title(f'{continente}\\n({len(dados_cont)} pa√≠ses)', \n",
        "                           fontweight='bold', fontsize=10)\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, f'{continente}\\nDados insuficientes', \n",
        "                      ha='center', va='center', fontsize=12)\n",
        "        axes[idx].set_xticks([])\n",
        "        axes[idx].set_yticks([])\n",
        "\n",
        "plt.suptitle('Matriz de Correla√ß√£o por Continente', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlacoes_por_continente.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Heatmaps de correla√ß√£o por continente salvos em 'correlacoes_por_continente.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dashboard de Top 10 em diferentes categorias\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìà DASHBOARD VISUAL: TOP 10 EM DIFERENTES CATEGORIAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# Top 10 Mais Ricos\n",
        "top10_ricos_viz = df_rankings.nlargest(10, 'GDP per capita. PPP (constant 2021 international $)')\n",
        "axes[0, 0].barh(range(len(top10_ricos_viz)), \n",
        "                top10_ricos_viz['GDP per capita. PPP (constant 2021 international $)'],\n",
        "                color='green', edgecolor='black', alpha=0.8)\n",
        "axes[0, 0].set_yticks(range(len(top10_ricos_viz)))\n",
        "axes[0, 0].set_yticklabels(top10_ricos_viz['Country Name'], fontsize=9)\n",
        "axes[0, 0].set_xlabel('PIB per Capita (USD)', fontsize=11)\n",
        "axes[0, 0].set_title('üí∞ Top 10 Pa√≠ses Mais Ricos', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
        "axes[0, 0].invert_yaxis()\n",
        "\n",
        "# Top 10 Mais Felizes\n",
        "top10_felizes_viz = df_rankings.nlargest(10, 'Cantril ladder score')\n",
        "axes[0, 1].barh(range(len(top10_felizes_viz)), \n",
        "                top10_felizes_viz['Cantril ladder score'],\n",
        "                color='gold', edgecolor='black', alpha=0.8)\n",
        "axes[0, 1].set_yticks(range(len(top10_felizes_viz)))\n",
        "axes[0, 1].set_yticklabels(top10_felizes_viz['Country Name'], fontsize=9)\n",
        "axes[0, 1].set_xlabel('Felicidade (Escada de Cantril)', fontsize=11)\n",
        "axes[0, 1].set_title('üòä Top 10 Pa√≠ses Mais Felizes', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
        "axes[0, 1].invert_yaxis()\n",
        "\n",
        "# Top 10 Maior IDH\n",
        "top10_idh_viz = df_rankings.nlargest(10, 'HDI (Human Development Indicator) Value (0~1000)')\n",
        "axes[1, 0].barh(range(len(top10_idh_viz)), \n",
        "                top10_idh_viz['HDI (Human Development Indicator) Value (0~1000)'],\n",
        "                color='steelblue', edgecolor='black', alpha=0.8)\n",
        "axes[1, 0].set_yticks(range(len(top10_idh_viz)))\n",
        "axes[1, 0].set_yticklabels(top10_idh_viz['Country Name'], fontsize=9)\n",
        "axes[1, 0].set_xlabel('IDH (0-1000)', fontsize=11)\n",
        "axes[1, 0].set_title('üåç Top 10 Pa√≠ses com Maior IDH', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# Top 10 Mais Seguros (menor taxa de homic√≠dios)\n",
        "top10_seguros_viz = df_rankings.nsmallest(10, 'Intentional homicides (per 100000 people) as of 2021')\n",
        "axes[1, 1].barh(range(len(top10_seguros_viz)), \n",
        "                100 - top10_seguros_viz['Intentional homicides (per 100000 people) as of 2021'],\n",
        "                color='lightgreen', edgecolor='black', alpha=0.8)\n",
        "axes[1, 1].set_yticks(range(len(top10_seguros_viz)))\n",
        "axes[1, 1].set_yticklabels(top10_seguros_viz['Country Name'], fontsize=9)\n",
        "axes[1, 1].set_xlabel('√çndice de Seguran√ßa (invertido)', fontsize=11)\n",
        "axes[1, 1].set_title('üõ°Ô∏è Top 10 Pa√≠ses Mais Seguros', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "axes[1, 1].invert_yaxis()\n",
        "\n",
        "plt.suptitle('Dashboard: Top 10 Rankings em Diferentes Categorias', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('dashboard_top10.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Dashboard de Top 10 salvo em 'dashboard_top10.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo Executivo das An√°lises Explorat√≥rias\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìã RESUMO EXECUTIVO DAS AN√ÅLISES EXPLORAT√ìRIAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "resumo = f\"\"\"\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "RESUMO EXECUTIVO: INSIGHTS EXTRA√çDOS DOS DADOS\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\n",
        "1. RANKING DE CONTINENTES:\n",
        "   ü•á {ranking_continentes.iloc[0]['Continente']} - Melhor score de desenvolvimento\n",
        "   ü•à {ranking_continentes.iloc[1]['Continente']} - Segundo melhor\n",
        "   ü•â {ranking_continentes.iloc[2]['Continente']} - Terceiro melhor\n",
        "\n",
        "2. INSIGHTS PRINCIPAIS:\n",
        "\n",
        "   üí∞ RIQUEZA:\n",
        "   - Pa√≠s mais rico: {top10_ricos.iloc[0]['Country Name']} (${top10_ricos.iloc[0]['GDP per capita. PPP (constant 2021 international $)']:,.0f})\n",
        "   - Pa√≠s mais pobre: {top10_pobres.iloc[0]['Country Name']} (${top10_pobres.iloc[0]['GDP per capita. PPP (constant 2021 international $)']:,.0f})\n",
        "   - Diferen√ßa: {top10_ricos.iloc[0]['GDP per capita. PPP (constant 2021 international $)'] / top10_pobres.iloc[0]['GDP per capita. PPP (constant 2021 international $)']:.1f}x\n",
        "\n",
        "   üòä FELICIDADE:\n",
        "   - Pa√≠s mais feliz: {top10_felizes.iloc[0]['Country Name']} (Felicidade: {top10_felizes.iloc[0]['Cantril ladder score']:.2f})\n",
        "   - Pa√≠s menos feliz: {top10_infelizes.iloc[0]['Country Name']} (Felicidade: {top10_infelizes.iloc[0]['Cantril ladder score']:.2f})\n",
        "   - Diferen√ßa: {top10_felizes.iloc[0]['Cantril ladder score'] - top10_infelizes.iloc[0]['Cantril ladder score']:.2f} pontos\n",
        "\n",
        "   üåç DESENVOLVIMENTO HUMANO:\n",
        "   - Maior IDH: {top10_idh.iloc[0]['Country Name']} (IDH: {top10_idh.iloc[0]['HDI (Human Development Indicator) Value (0~1000)']:.0f})\n",
        "   - Menor IDH: {top10_idh_baixo.iloc[0]['Country Name']} (IDH: {top10_idh_baixo.iloc[0]['HDI (Human Development Indicator) Value (0~1000)']:.0f})\n",
        "\n",
        "   üõ°Ô∏è SEGURAN√áA:\n",
        "   - Pa√≠s mais seguro: {top10_seguros.iloc[0]['Country Name']} ({top10_seguros.iloc[0]['Intentional homicides (per 100000 people) as of 2021']:.2f} homic√≠dios/100k)\n",
        "   - Pa√≠s mais violento: {top10_violentos.iloc[0]['Country Name']} ({top10_violentos.iloc[0]['Intentional homicides (per 100000 people) as of 2021']:.2f} homic√≠dios/100k)\n",
        "   - Diferen√ßa: {top10_violentos.iloc[0]['Intentional homicides (per 100000 people) as of 2021'] / top10_seguros.iloc[0]['Intentional homicides (per 100000 people) as of 2021']:.1f}x mais violento\n",
        "\n",
        "3. DESCOBERTAS INTERESSANTES:\n",
        "\n",
        "   üí° Dinheiro vs Felicidade:\n",
        "   - Pa√≠ses ricos mas menos felizes: {', '.join(ricos_infelizes['Country Name'].head(3).tolist())}\n",
        "   - Pa√≠ses pobres mas mais felizes: {', '.join(pobres_felizes['Country Name'].head(3).tolist())}\n",
        "   - Conclus√£o: Riqueza n√£o garante felicidade, e felicidade n√£o requer riqueza extrema\n",
        "\n",
        "   üèÜ Melhor Equil√≠brio:\n",
        "   - Top 3 pa√≠ses com melhor equil√≠brio geral: {', '.join(top_equilibrio['Country Name'].head(3).tolist())}\n",
        "   - Estes pa√≠ses combinam alta riqueza, felicidade, desenvolvimento humano e seguran√ßa\n",
        "\n",
        "4. PODER DA CI√äNCIA DE DADOS:\n",
        "\n",
        "   ‚úÖ Esta an√°lise demonstra como dados podem revelar:\n",
        "   - Padr√µes geogr√°ficos (diferen√ßas entre continentes)\n",
        "   - Rela√ß√µes complexas (riqueza vs felicidade)\n",
        "   - Rankings objetivos em m√∫ltiplas dimens√µes\n",
        "   - Exce√ß√µes interessantes (pa√≠ses que desafiam expectativas)\n",
        "   - Oportunidades para pol√≠ticas p√∫blicas baseadas em evid√™ncias\n",
        "\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "print(resumo)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
