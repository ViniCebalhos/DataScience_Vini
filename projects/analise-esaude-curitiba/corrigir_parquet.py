#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para corrigir o problema do parquet no notebook
Substitui o c√≥digo problem√°tico por uma vers√£o que trata a aus√™ncia das depend√™ncias
"""

def corrigir_codigo_parquet():
    """
    Retorna o c√≥digo corrigido para substituir no notebook
    """
    
    codigo_corrigido = '''
# Salva o dataset consolidado para uso futuro
print("Salvando dataset consolidado...")

# Remove colunas tempor√°rias criadas para an√°lise
colunas_para_remover = ['Ano', 'Mes', 'Dia', 'DiaSemana', 'Hora', 'Semana']
df_consolidado_final = df_completo.drop(columns=colunas_para_remover, errors='ignore')

# Salva em formato CSV
arquivo_saida = 'Dados/dataset_consolidado_completo.csv'
df_consolidado_final.to_csv(arquivo_saida, index=False, sep=';', encoding='utf-8')

print(f"‚úì Dataset consolidado salvo em: {arquivo_saida}")
print(f"  - {len(df_consolidado_final):,} registros")
print(f"  - {df_consolidado_final.shape[1]} colunas")
print(f"  - Tamanho do arquivo: {os.path.getsize(arquivo_saida) / (1024*1024):.1f} MB")

# Tenta salvar em formato parquet (mais eficiente)
try:
    arquivo_parquet = 'Dados/dataset_consolidado_completo.parquet'
    df_consolidado_final.to_parquet(arquivo_parquet, index=False)
    print(f"‚úì Dataset tamb√©m salvo em formato parquet: {arquivo_parquet}")
    print(f"  - Tamanho do arquivo: {os.path.getsize(arquivo_parquet) / (1024*1024):.1f} MB")
except ImportError as e:
    print("‚ö†Ô∏è  Formato parquet n√£o dispon√≠vel (pyarrow ou fastparquet n√£o instalados)")
    print("   Para instalar: pip install pyarrow fastparquet")
    print("   Dataset salvo apenas em formato CSV")
except Exception as e:
    print(f"‚ö†Ô∏è  Erro ao salvar em parquet: {e}")
    print("   Dataset salvo apenas em formato CSV")
'''
    
    return codigo_corrigido

def instrucoes_instalacao():
    """
    Retorna instru√ß√µes para instalar as depend√™ncias do parquet
    """
    
    instrucoes = '''
üîß INSTRU√á√ïES PARA INSTALAR DEPEND√äNCIAS DO PARQUET:

1. **Instalar pyarrow (recomendado):**
   ```bash
   pip install pyarrow
   ```

2. **Ou instalar fastparquet (alternativa):**
   ```bash
   pip install fastparquet
   ```

3. **Ou instalar ambas (mais seguro):**
   ```bash
   pip install pyarrow fastparquet
   ```

4. **Se estiver usando conda:**
   ```bash
   conda install pyarrow
   # ou
   conda install -c conda-forge pyarrow
   ```

5. **Verificar se foi instalado:**
   ```python
   import pyarrow
   print("pyarrow instalado com sucesso!")
   ```

‚ö†Ô∏è  **NOTA:** Se n√£o conseguir instalar, o dataset ser√° salvo apenas em formato CSV, 
que tamb√©m funciona perfeitamente para a an√°lise.
'''
    
    return instrucoes

def solucao_alternativa():
    """
    Retorna uma solu√ß√£o alternativa que n√£o depende do parquet
    """
    
    solucao = '''
üîß SOLU√á√ÉO ALTERNATIVA (SEM PARQUET):

Se voc√™ n√£o conseguir instalar pyarrow ou fastparquet, use este c√≥digo:

```python
# Salva o dataset consolidado para uso futuro
print("Salvando dataset consolidado...")

# Remove colunas tempor√°rias criadas para an√°lise
colunas_para_remover = ['Ano', 'Mes', 'Dia', 'DiaSemana', 'Hora', 'Semana']
df_consolidado_final = df_completo.drop(columns=colunas_para_remover, errors='ignore')

# Salva em formato CSV
arquivo_saida = 'Dados/dataset_consolidado_completo.csv'
df_consolidado_final.to_csv(arquivo_saida, index=False, sep=';', encoding='utf-8')

print(f"‚úì Dataset consolidado salvo em: {arquivo_saida}")
print(f"  - {len(df_consolidado_final):,} registros")
print(f"  - {df_consolidado_final.shape[1]} colunas")
print(f"  - Tamanho do arquivo: {os.path.getsize(arquivo_saida) / (1024*1024):.1f} MB")

# Salva tamb√©m em formato pickle (alternativa ao parquet)
try:
    arquivo_pickle = 'Dados/dataset_consolidado_completo.pkl'
    df_consolidado_final.to_pickle(arquivo_pickle)
    print(f"‚úì Dataset tamb√©m salvo em formato pickle: {arquivo_pickle}")
    print(f"  - Tamanho do arquivo: {os.path.getsize(arquivo_pickle) / (1024*1024):.1f} MB")
except Exception as e:
    print(f"‚ö†Ô∏è  Erro ao salvar em pickle: {e}")
    print("   Dataset salvo apenas em formato CSV")
```

‚úÖ **Vantagens do CSV:**
- Formato universal, funciona em qualquer sistema
- Pode ser aberto em Excel, Google Sheets, etc.
- N√£o requer depend√™ncias especiais
- F√°cil de compartilhar e versionar

‚ö†Ô∏è  **Desvantagens do CSV:**
- Arquivo maior que parquet
- Leitura mais lenta para datasets grandes
- N√£o preserva tipos de dados complexos
'''
    
    return solucao

if __name__ == "__main__":
    print("üîß CORRE√á√ÉO DO PROBLEMA DO PARQUET")
    print("=" * 50)
    print()
    
    print("‚ùå PROBLEMA:")
    print("ImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'")
    print()
    
    print("‚úÖ SOLU√á√ÉO:")
    print("Substitua o c√≥digo problem√°tico no notebook por:")
    print()
    print(corrigir_codigo_parquet())
    
    print("üìã INSTRU√á√ïES DE INSTALA√á√ÉO:")
    print(instrucoes_instalacao())
    
    print("üîÑ SOLU√á√ÉO ALTERNATIVA:")
    print(solucao_alternativa())
    
    print("üéØ RECOMENDA√á√ÉO:")
    print("1. Tente instalar pyarrow primeiro")
    print("2. Se n√£o conseguir, use a solu√ß√£o alternativa com CSV + pickle")
    print("3. O formato CSV √© suficiente para a maioria das an√°lises") 