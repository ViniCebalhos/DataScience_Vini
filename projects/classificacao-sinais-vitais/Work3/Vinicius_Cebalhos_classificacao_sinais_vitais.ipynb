{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifica√ß√£o de Sinais Vitais - Resgate de V√≠timas de Cat√°strofes\n",
    "\n",
    "**Trabalho 3 - Curso de Especializa√ß√£o em Ci√™ncia de Dados 2025**\n",
    "\n",
    "Aluno: Vin√≠cius de Souza Cebalhos\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Classificar v√≠timas de acidentes em 4 classes de gravidade baseado em sinais vitais:\n",
    "- **Classe 1**: Cr√≠tico\n",
    "- **Classe 2**: Inst√°vel  \n",
    "- **Classe 3**: Potencialmente Est√°vel\n",
    "- **Classe 4**: Est√°vel\n",
    "\n",
    "## Tarefa\n",
    "\n",
    "Comparar dois modelos de classifica√ß√£o:\n",
    "1. **Random Forest** (Florestas Aleat√≥rias)\n",
    "2. **MLP** (Multi-Layer Perceptron - Rede Neural Artificial)\n",
    "\n",
    "Usando m√©tricas: Precis√£o, Recall, F1-Score, Acur√°cia e Matriz de Confus√£o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                            confusion_matrix, classification_report)\n",
    "import warnings\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Auditoria dos Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento do arquivo de treino\n",
    "print(\"=\" * 80)\n",
    "print(\"CARREGAMENTO DOS DADOS DE TREINO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_path = 'treino_sinais_vitais_com_label_1500.txt'\n",
    "\n",
    "try:\n",
    "    # Tentar carregar o arquivo\n",
    "    # Formato: i,si1,si2,si3,si4,si5,gi,yi (separado por v√≠rgulas)\n",
    "    df_train = pd.read_csv(file_path, sep=',', header=None, \n",
    "                          names=['i', 'si1', 'si2', 'si3', 'si4', 'si5', 'gi', 'yi'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Arquivo carregado com sucesso!\")\n",
    "    print(f\"Shape: {df_train.shape}\")\n",
    "    print(f\"Linhas: {df_train.shape[0]}, Colunas: {df_train.shape[1]}\\n\")\n",
    "    \n",
    "    print(\"Primeiras linhas do dataset:\")\n",
    "    print(df_train.head(10))\n",
    "    \n",
    "    print(\"\\n√öltimas linhas do dataset:\")\n",
    "    print(df_train.tail(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Arquivo '{file_path}' n√£o encontrado!\")\n",
    "    print(\"Por favor, certifique-se de que o arquivo est√° no diret√≥rio correto.\")\n",
    "    print(\"\\nEstrutura esperada do arquivo:\")\n",
    "    print(\"i si1 si2 si3 si4 si5 gi yi\")\n",
    "    print(\"1 valor1 valor2 valor3 valor4 valor5 valor6 classe\")\n",
    "    df_train = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao carregar arquivo: {e}\")\n",
    "    df_train = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auditoria inicial dos dados\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AUDITORIA INICIAL DOS DADOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. INFORMA√á√ïES GERAIS:\")\n",
    "    print(df_train.info())\n",
    "    \n",
    "    print(\"\\n2. ESTAT√çSTICAS DESCRITIVAS:\")\n",
    "    print(df_train.describe())\n",
    "    \n",
    "    print(\"\\n3. VERIFICA√á√ÉO DE VALORES AUSENTES:\")\n",
    "    missing = df_train.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Valores ausentes encontrados:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhum valor ausente encontrado\")\n",
    "    \n",
    "    print(\"\\n4. VERIFICA√á√ÉO DE DUPLICATAS:\")\n",
    "    duplicatas = df_train.duplicated().sum()\n",
    "    if duplicatas > 0:\n",
    "        print(f\"‚ö†Ô∏è {duplicatas} linhas duplicadas encontradas\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhuma linha duplicada\")\n",
    "    \n",
    "    print(\"\\n5. DISTRIBUI√á√ÉO DAS CLASSES (yi):\")\n",
    "    print(df_train['yi'].value_counts().sort_index())\n",
    "    print(f\"\\nDistribui√ß√£o percentual:\")\n",
    "    print((df_train['yi'].value_counts(normalize=True) * 100).sort_index())\n",
    "    \n",
    "    print(\"\\n6. VERIFICA√á√ÉO DE FAIXAS ESPERADAS:\")\n",
    "    print(\"\\nsi1 (press√£o sist√≥lica): esperado [5, 22]\")\n",
    "    print(f\"  Observado: [{df_train['si1'].min():.2f}, {df_train['si1'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\nsi2 (press√£o diast√≥lica): esperado [0, 15]\")\n",
    "    print(f\"  Observado: [{df_train['si2'].min():.2f}, {df_train['si2'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\nsi3 (qPA - qualidade press√£o): esperado [-10, 10]\")\n",
    "    print(f\"  Observado: [{df_train['si3'].min():.2f}, {df_train['si3'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\nsi4 (pulso): esperado [0, 200] bpm\")\n",
    "    print(f\"  Observado: [{df_train['si4'].min():.2f}, {df_train['si4'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\nsi5 (respira√ß√£o): esperado [0, 22] FpM\")\n",
    "    print(f\"  Observado: [{df_train['si5'].min():.2f}, {df_train['si5'].max():.2f}]\")\n",
    "    \n",
    "    print(\"\\n7. VERIFICA√á√ÉO DE VALORES NEGATIVOS (onde n√£o deveria haver):\")\n",
    "    if (df_train['si4'] < 0).sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è {((df_train['si4'] < 0).sum())} valores negativos em si4 (pulso)\")\n",
    "    else:\n",
    "        print(\"‚úÖ si4 (pulso): nenhum valor negativo\")\n",
    "    \n",
    "    if (df_train['si5'] < 0).sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è {((df_train['si5'] < 0).sum())} valores negativos em si5 (respira√ß√£o)\")\n",
    "    else:\n",
    "        print(\"‚úÖ si5 (respira√ß√£o): nenhum valor negativo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise Explorat√≥ria\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"AN√ÅLISE EXPLORAT√ìRIA DOS DADOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Features que DEVEM ser usadas (conforme enunciado)\n",
    "    features_usar = ['si3', 'si4', 'si5', 'gi']  # si1 e si2 N√ÉO devem ser usadas\n",
    "    \n",
    "    print(\"\\nüìå IMPORTANTE: Conforme o enunciado:\")\n",
    "    print(\"   - si1 (press√£o sist√≥lica): N√ÉO USAR (usada apenas no c√°lculo de si3)\")\n",
    "    print(\"   - si2 (press√£o diast√≥lica): N√ÉO USAR (usada apenas no c√°lculo de si3)\")\n",
    "    print(\"   - si3 (qPA): USAR\")\n",
    "    print(\"   - si4 (pulso): USAR\")\n",
    "    print(\"   - si5 (respira√ß√£o): USAR\")\n",
    "    print(\"   - gi (gravidade): USAR\")\n",
    "    \n",
    "    print(f\"\\nFeatures selecionadas para modelagem: {features_usar}\")\n",
    "    \n",
    "    # Estat√≠sticas descritivas das features selecionadas\n",
    "    print(\"\\nEstat√≠sticas descritivas das features selecionadas:\")\n",
    "    print(df_train[features_usar].describe())\n",
    "    \n",
    "    # Correla√ß√£o entre features\n",
    "    print(\"\\nMatriz de correla√ß√£o entre features:\")\n",
    "    corr_matrix = df_train[features_usar].corr()\n",
    "    print(corr_matrix)\n",
    "    \n",
    "    # Visualiza√ß√£o da correla√ß√£o\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Matriz de Correla√ß√£o entre Features', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlacao_features.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Heatmap de correla√ß√£o salvo em 'correlacao_features.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√µes das features por classe\n",
    "if df_train is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUI√á√ïES DAS FEATURES POR CLASSE DE GRAVIDADE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Criar visualiza√ß√µes\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, feature in enumerate(features_usar):\n",
    "        for classe in sorted(df_train['yi'].unique()):\n",
    "            dados_classe = df_train[df_train['yi'] == classe][feature]\n",
    "            axes[idx].hist(dados_classe, alpha=0.6, label=f'Classe {int(classe)}', bins=30)\n",
    "        \n",
    "        axes[idx].set_title(f'Distribui√ß√£o de {feature} por Classe', fontweight='bold')\n",
    "        axes[idx].set_xlabel(feature)\n",
    "        axes[idx].set_ylabel('Frequ√™ncia')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Distribui√ß√µes das Features por Classe de Gravidade', \n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribuicoes_por_classe.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Gr√°ficos de distribui√ß√£o salvos em 'distribuicoes_por_classe.png'\")\n",
    "    \n",
    "    # Estat√≠sticas por classe\n",
    "    print(\"\\nEstat√≠sticas das features por classe:\")\n",
    "    for classe in sorted(df_train['yi'].unique()):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLASSE {int(classe)}: {['Cr√≠tico', 'Inst√°vel', 'Potencialmente Est√°vel', 'Est√°vel'][int(classe)-1]}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        dados_classe = df_train[df_train['yi'] == classe][features_usar]\n",
    "        print(dados_classe.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepara√ß√£o dos Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara√ß√£o dos dados\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PREPARA√á√ÉO DOS DADOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Selecionar apenas as features que DEVEM ser usadas\n",
    "    X = df_train[features_usar].copy()\n",
    "    y = df_train['yi'].astype(int).copy()\n",
    "    \n",
    "    print(f\"\\nFeatures selecionadas: {features_usar}\")\n",
    "    print(f\"Shape de X: {X.shape}\")\n",
    "    print(f\"Shape de y: {y.shape}\")\n",
    "    print(f\"\\nDistribui√ß√£o das classes:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    \n",
    "    # Verificar balanceamento\n",
    "    balance_ratio = y.value_counts().min() / y.value_counts().max()\n",
    "    print(f\"\\nRaz√£o de balanceamento: {balance_ratio:.3f}\")\n",
    "    if balance_ratio < 0.5:\n",
    "        print(\"‚ö†Ô∏è Dataset desbalanceado detectado\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset relativamente balanceado\")\n",
    "    \n",
    "    # Separa√ß√£o treino/valida√ß√£o (80/20)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEPARA√á√ÉO TREINO/VALIDA√á√ÉO\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Estrat√©gia: 80% treino, 20% valida√ß√£o (stratified)\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Separa√ß√£o realizada:\")\n",
    "    print(f\"  Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "    print(f\"  Valida√ß√£o: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n  Distribui√ß√£o de classes no TREINO:\")\n",
    "    print(y_train.value_counts().sort_index())\n",
    "    print(f\"\\n  Distribui√ß√£o de classes na VALIDA√á√ÉO:\")\n",
    "    print(y_val.value_counts().sort_index())\n",
    "    \n",
    "    # Normaliza√ß√£o (importante para MLP)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NORMALIZA√á√ÉO DOS DADOS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Estrat√©gia: StandardScaler (importante para MLP)\")\n",
    "    print(\"Justificativa: MLP √© sens√≠vel √† escala das features\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Converter de volta para DataFrame\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features_usar, index=X_train.index)\n",
    "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=features_usar, index=X_val.index)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Normaliza√ß√£o realizada\")\n",
    "    print(f\"  M√©dias ap√≥s normaliza√ß√£o (treino): {X_train_scaled.mean().round(3).to_dict()}\")\n",
    "    print(f\"  Desvios padr√£o ap√≥s normaliza√ß√£o (treino): {X_train_scaled.std().round(3).to_dict()}\")\n",
    "    \n",
    "    # Para Random Forest, n√£o precisa normaliza√ß√£o, mas n√£o prejudica\n",
    "    # Vamos usar dados normalizados para ambos para facilitar compara√ß√£o\n",
    "    X_train_final = X_train_scaled\n",
    "    X_val_final = X_val_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ajuste dos Modelos de Classifica√ß√£o\n",
    "\n",
    "### 4.1 Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 1: Random Forest\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODELO 1: RANDOM FOREST (FLORESTAS ALEAT√ìRIAS)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Configurar Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPar√¢metros do modelo:\")\n",
    "    print(f\"  - n_estimators: {rf.n_estimators}\")\n",
    "    print(f\"  - max_depth: {rf.max_depth}\")\n",
    "    print(f\"  - random_state: {rf.random_state}\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"VALIDA√á√ÉO CRUZADA (5-fold estratificado)\")\n",
    "    print(\"-\"*80)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores_rf = cross_val_score(rf, X_train_final, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"Accuracy m√©dia (CV): {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")\n",
    "    \n",
    "    # Treinar no conjunto completo de treino\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TREINAMENTO DO MODELO\")\n",
    "    print(\"-\"*80)\n",
    "    rf.fit(X_train_final, y_train)\n",
    "    print(\"‚úÖ Modelo treinado com sucesso!\")\n",
    "    \n",
    "    # Predi√ß√µes no conjunto de valida√ß√£o\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PREDI√á√ïES NO CONJUNTO DE VALIDA√á√ÉO\")\n",
    "    print(\"-\"*80)\n",
    "    y_pred_rf = rf.predict(X_val_final)\n",
    "    y_pred_proba_rf = rf.predict_proba(X_val_final)\n",
    "    \n",
    "    # M√©tricas\n",
    "    acc_rf = accuracy_score(y_val, y_pred_rf)\n",
    "    prec_rf = precision_score(y_val, y_pred_rf, average='weighted', zero_division=0)\n",
    "    rec_rf = recall_score(y_val, y_pred_rf, average='weighted', zero_division=0)\n",
    "    f1_rf = f1_score(y_val, y_pred_rf, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DO RANDOM FOREST:\")\n",
    "    print(f\"  Accuracy:  {acc_rf:.4f}\")\n",
    "    print(f\"  Precision: {prec_rf:.4f}\")\n",
    "    print(f\"  Recall:    {rec_rf:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1_rf:.4f}\")\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm_rf = confusion_matrix(y_val, y_pred_rf)\n",
    "    print(f\"\\nüìã MATRIZ DE CONFUS√ÉO:\")\n",
    "    print(cm_rf)\n",
    "    \n",
    "    # Feature importance\n",
    "    print(f\"\\nüîç FEATURE IMPORTANCE:\")\n",
    "    feature_importance_rf = pd.DataFrame({\n",
    "        'Feature': features_usar,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(feature_importance_rf.to_string(index=False))\n",
    "    \n",
    "    # Salvar resultados\n",
    "    resultados_rf = {\n",
    "        'model': rf,\n",
    "        'accuracy': acc_rf,\n",
    "        'precision': prec_rf,\n",
    "        'recall': rec_rf,\n",
    "        'f1': f1_rf,\n",
    "        'cv_scores': cv_scores_rf,\n",
    "        'y_pred': y_pred_rf,\n",
    "        'y_pred_proba': y_pred_proba_rf,\n",
    "        'confusion_matrix': cm_rf,\n",
    "        'feature_importance': feature_importance_rf\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO 2: MLP (Multi-Layer Perceptron)\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODELO 2: MLP (MULTI-LAYER PERCEPTRON - REDE NEURAL)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Configurar MLP\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),  # 2 camadas ocultas: 100 e 50 neur√¥nios\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.0001,  # Regulariza√ß√£o L2\n",
    "        batch_size='auto',\n",
    "        learning_rate='constant',\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPar√¢metros do modelo:\")\n",
    "    print(f\"  - hidden_layer_sizes: {mlp.hidden_layer_sizes}\")\n",
    "    print(f\"  - activation: {mlp.activation}\")\n",
    "    print(f\"  - solver: {mlp.solver}\")\n",
    "    print(f\"  - max_iter: {mlp.max_iter}\")\n",
    "    print(f\"  - early_stopping: {mlp.early_stopping}\")\n",
    "    \n",
    "    # Valida√ß√£o cruzada\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"VALIDA√á√ÉO CRUZADA (5-fold estratificado)\")\n",
    "    print(\"-\"*80)\n",
    "    cv_scores_mlp = cross_val_score(mlp, X_train_final, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"Accuracy m√©dia (CV): {cv_scores_mlp.mean():.4f} (+/- {cv_scores_mlp.std() * 2:.4f})\")\n",
    "    \n",
    "    # Treinar no conjunto completo de treino\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TREINAMENTO DO MODELO\")\n",
    "    print(\"-\"*80)\n",
    "    mlp.fit(X_train_final, y_train)\n",
    "    print(\"‚úÖ Modelo treinado com sucesso!\")\n",
    "    print(f\"  Itera√ß√µes realizadas: {mlp.n_iter_}\")\n",
    "    print(f\"  Loss final: {mlp.loss_:.4f}\")\n",
    "    \n",
    "    # Predi√ß√µes no conjunto de valida√ß√£o\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PREDI√á√ïES NO CONJUNTO DE VALIDA√á√ÉO\")\n",
    "    print(\"-\"*80)\n",
    "    y_pred_mlp = mlp.predict(X_val_final)\n",
    "    y_pred_proba_mlp = mlp.predict_proba(X_val_final)\n",
    "    \n",
    "    # M√©tricas\n",
    "    acc_mlp = accuracy_score(y_val, y_pred_mlp)\n",
    "    prec_mlp = precision_score(y_val, y_pred_mlp, average='weighted', zero_division=0)\n",
    "    rec_mlp = recall_score(y_val, y_pred_mlp, average='weighted', zero_division=0)\n",
    "    f1_mlp = f1_score(y_val, y_pred_mlp, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüìä M√âTRICAS DO MLP:\")\n",
    "    print(f\"  Accuracy:  {acc_mlp:.4f}\")\n",
    "    print(f\"  Precision: {prec_mlp:.4f}\")\n",
    "    print(f\"  Recall:    {rec_mlp:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1_mlp:.4f}\")\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm_mlp = confusion_matrix(y_val, y_pred_mlp)\n",
    "    print(f\"\\nüìã MATRIZ DE CONFUS√ÉO:\")\n",
    "    print(cm_mlp)\n",
    "    \n",
    "    # Salvar resultados\n",
    "    resultados_mlp = {\n",
    "        'model': mlp,\n",
    "        'accuracy': acc_mlp,\n",
    "        'precision': prec_mlp,\n",
    "        'recall': rec_mlp,\n",
    "        'f1': f1_mlp,\n",
    "        'cv_scores': cv_scores_mlp,\n",
    "        'y_pred': y_pred_mlp,\n",
    "        'y_pred_proba': y_pred_proba_mlp,\n",
    "        'confusion_matrix': cm_mlp\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compara√ß√£o dos Modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compara√ß√£o dos Modelos\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPARA√á√ÉO DOS MODELOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Tabela comparativa\n",
    "    comparacao = pd.DataFrame({\n",
    "        'Modelo': ['Random Forest', 'MLP'],\n",
    "        'Accuracy': [resultados_rf['accuracy'], resultados_mlp['accuracy']],\n",
    "        'Precision': [resultados_rf['precision'], resultados_mlp['precision']],\n",
    "        'Recall': [resultados_rf['recall'], resultados_mlp['recall']],\n",
    "        'F1-Score': [resultados_rf['f1'], resultados_mlp['f1']],\n",
    "        'CV Accuracy (m√©dia)': [resultados_rf['cv_scores'].mean(), resultados_mlp['cv_scores'].mean()],\n",
    "        'CV Accuracy (std)': [resultados_rf['cv_scores'].std(), resultados_mlp['cv_scores'].std()]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nüìä TABELA COMPARATIVA:\")\n",
    "    print(comparacao.to_string(index=False))\n",
    "    \n",
    "    # Identificar melhor modelo\n",
    "    melhor_modelo = comparacao.loc[comparacao['F1-Score'].idxmax(), 'Modelo']\n",
    "    print(f\"\\nüèÜ MELHOR MODELO (por F1-Score): {melhor_modelo}\")\n",
    "    \n",
    "    # Visualiza√ß√£o: Matrizes de confus√£o lado a lado\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Random Forest\n",
    "    sns.heatmap(resultados_rf['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "                ax=axes[0], xticklabels=sorted(y_val.unique()), yticklabels=sorted(y_val.unique()))\n",
    "    axes[0].set_title(f'Random Forest\\nAccuracy: {resultados_rf[\"accuracy\"]:.3f}', fontweight='bold')\n",
    "    axes[0].set_xlabel('Predito')\n",
    "    axes[0].set_ylabel('Real')\n",
    "    \n",
    "    # MLP\n",
    "    sns.heatmap(resultados_mlp['confusion_matrix'], annot=True, fmt='d', cmap='Oranges', \n",
    "                ax=axes[1], xticklabels=sorted(y_val.unique()), yticklabels=sorted(y_val.unique()))\n",
    "    axes[1].set_title(f'MLP\\nAccuracy: {resultados_mlp[\"accuracy\"]:.3f}', fontweight='bold')\n",
    "    axes[1].set_xlabel('Predito')\n",
    "    axes[1].set_ylabel('Real')\n",
    "    \n",
    "    plt.suptitle('Matrizes de Confus√£o - Compara√ß√£o dos Modelos', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('matrizes_confusao_comparacao.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Matrizes de confus√£o salvas em 'matrizes_confusao_comparacao.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relat√≥rios de classifica√ß√£o detalhados\n",
    "if df_train is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RELAT√ìRIOS DE CLASSIFICA√á√ÉO DETALHADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    class_names = ['Cr√≠tico (1)', 'Inst√°vel (2)', 'Pot. Est√°vel (3)', 'Est√°vel (4)']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RANDOM FOREST\")\n",
    "    print(\"=\"*80)\n",
    "    print(classification_report(y_val, resultados_rf['y_pred'], \n",
    "                                target_names=class_names))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MLP (REDE NEURAL)\")\n",
    "    print(\"=\"*80)\n",
    "    print(classification_report(y_val, resultados_mlp['y_pred'], \n",
    "                                target_names=class_names))\n",
    "    \n",
    "    # Visualiza√ß√£o comparativa de m√©tricas\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    modelos = ['Random Forest', 'MLP']\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    rf_values = [resultados_rf['accuracy'], resultados_rf['precision'], \n",
    "                 resultados_rf['recall'], resultados_rf['f1']]\n",
    "    mlp_values = [resultados_mlp['accuracy'], resultados_mlp['precision'], \n",
    "                  resultados_mlp['recall'], resultados_mlp['f1']]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, rf_values, width, label='Random Forest', color='steelblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, mlp_values, width, label='MLP', color='orange', alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Compara√ß√£o de M√©tricas: Random Forest vs MLP', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacao_metricas.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Gr√°fico comparativo de m√©tricas salvo em 'comparacao_metricas.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aplica√ß√£o no Teste Cego\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar e processar dados de teste cego\n",
    "print(\"=\" * 80)\n",
    "print(\"APLICA√á√ÉO NO TESTE CEGO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "file_test = 'teste_cego_com_classe.csv'\n",
    "\n",
    "try:\n",
    "    # Carregar arquivo de teste (sem labels)\n",
    "    # Formato: i,si3,si4,si5,gi (separado por v√≠rgulas, sem si1, si2, yi)\n",
    "    df_test = pd.read_csv(file_test, sep=',', header=None, \n",
    "                         names=['i', 'si3', 'si4', 'si5', 'gi'])\n",
    "    \n",
    "    print(f\"\\n‚úÖ Arquivo de teste carregado com sucesso!\")\n",
    "    print(f\"Shape: {df_test.shape}\")\n",
    "    print(f\"Linhas: {df_test.shape[0]}, Colunas: {df_test.shape[1]}\\n\")\n",
    "    \n",
    "    print(\"Primeiras linhas do dataset de teste:\")\n",
    "    print(df_test.head(10))\n",
    "    \n",
    "    # Preparar dados de teste (usar as mesmas features)\n",
    "    X_test = df_test[features_usar].copy()\n",
    "    \n",
    "    print(f\"\\nFeatures selecionadas para teste: {features_usar}\")\n",
    "    print(f\"Shape de X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Normalizar usando o mesmo scaler do treino\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features_usar, index=X_test.index)\n",
    "    \n",
    "    print(\"\\n‚úÖ Dados de teste normalizados usando o scaler do treino\")\n",
    "    \n",
    "    # Fazer predi√ß√µes com ambos os modelos\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PREDI√á√ïES NO TESTE CEGO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Random Forest\n",
    "    y_pred_test_rf = rf.predict(X_test_scaled)\n",
    "    print(f\"\\nüìä Random Forest:\")\n",
    "    print(f\"  Total de predi√ß√µes: {len(y_pred_test_rf)}\")\n",
    "    print(f\"  Distribui√ß√£o das predi√ß√µes:\")\n",
    "    print(pd.Series(y_pred_test_rf).value_counts().sort_index())\n",
    "    \n",
    "    # MLP\n",
    "    y_pred_test_mlp = mlp.predict(X_test_scaled)\n",
    "    print(f\"\\nüìä MLP:\")\n",
    "    print(f\"  Total de predi√ß√µes: {len(y_pred_test_mlp)}\")\n",
    "    print(f\"  Distribui√ß√£o das predi√ß√µes:\")\n",
    "    print(pd.Series(y_pred_test_mlp).value_counts().sort_index())\n",
    "    \n",
    "    # Comparar predi√ß√µes dos dois modelos\n",
    "    print(f\"\\nüìä COMPARA√á√ÉO DAS PREDI√á√ïES:\")\n",
    "    concordancia = (y_pred_test_rf == y_pred_test_mlp).sum()\n",
    "    discordancia = (y_pred_test_rf != y_pred_test_mlp).sum()\n",
    "    print(f\"  Predi√ß√µes concordantes: {concordancia} ({concordancia/len(y_pred_test_rf)*100:.1f}%)\")\n",
    "    print(f\"  Predi√ß√µes discordantes: {discordancia} ({discordancia/len(y_pred_test_rf)*100:.1f}%)\")\n",
    "    \n",
    "    # Salvar predi√ß√µes\n",
    "    resultados_teste = pd.DataFrame({\n",
    "        'i': df_test['i'],\n",
    "        'si3': df_test['si3'],\n",
    "        'si4': df_test['si4'],\n",
    "        'si5': df_test['si5'],\n",
    "        'gi': df_test['gi'],\n",
    "        'predicao_RF': y_pred_test_rf,\n",
    "        'predicao_MLP': y_pred_test_mlp\n",
    "    })\n",
    "    \n",
    "    # Salvar em arquivo\n",
    "    resultados_teste.to_csv('predicoes_teste_cego.csv', index=False)\n",
    "    print(f\"\\n‚úÖ Predi√ß√µes salvas em 'predicoes_teste_cego.csv'\")\n",
    "    \n",
    "    # Mostrar primeiras predi√ß√µes\n",
    "    print(\"\\nPrimeiras 20 predi√ß√µes:\")\n",
    "    print(resultados_teste.head(20).to_string(index=False))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è Arquivo '{file_test}' n√£o encontrado!\")\n",
    "    print(\"O arquivo de teste ser√° processado quando estiver dispon√≠vel.\")\n",
    "    df_test = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao processar arquivo de teste: {e}\")\n",
    "    df_test = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resumo e Conclus√µes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo Final e Conclus√µes\n",
    "if df_train is not None:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"RESUMO FINAL E CONCLUS√ïES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    resumo = f\"\"\"\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "RESUMO DA AN√ÅLISE DE CLASSIFICA√á√ÉO DE SINAI VITAIS\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "1. OBJETIVO:\n",
    "   Classificar v√≠timas de acidentes em 4 classes de gravidade baseado em sinais vitais:\n",
    "   - Classe 1: Cr√≠tico\n",
    "   - Classe 2: Inst√°vel\n",
    "   - Classe 3: Potencialmente Est√°vel\n",
    "   - Classe 4: Est√°vel\n",
    "\n",
    "2. FEATURES UTILIZADAS:\n",
    "   - si3: Qualidade da press√£o arterial (qPA) [-10, 10]\n",
    "   - si4: Pulso [0, 200] bpm\n",
    "   - si5: Frequ√™ncia respirat√≥ria [0, 22] FpM\n",
    "   - gi: Gravidade (valor calculado)\n",
    "\n",
    "3. MODELOS AJUSTADOS:\n",
    "   \n",
    "   a) RANDOM FOREST:\n",
    "      - Accuracy:  {resultados_rf['accuracy']:.4f}\n",
    "      - Precision: {resultados_rf['precision']:.4f}\n",
    "      - Recall:    {resultados_rf['recall']:.4f}\n",
    "      - F1-Score:  {resultados_rf['f1']:.4f}\n",
    "      - CV Accuracy: {resultados_rf['cv_scores'].mean():.4f} ¬± {resultados_rf['cv_scores'].std():.4f}\n",
    "   \n",
    "   b) MLP (REDE NEURAL):\n",
    "      - Accuracy:  {resultados_mlp['accuracy']:.4f}\n",
    "      - Precision: {resultados_mlp['precision']:.4f}\n",
    "      - Recall:    {resultados_mlp['recall']:.4f}\n",
    "      - F1-Score:  {resultados_mlp['f1']:.4f}\n",
    "      - CV Accuracy: {resultados_mlp['cv_scores'].mean():.4f} ¬± {resultados_mlp['cv_scores'].std():.4f}\n",
    "\n",
    "4. COMPARA√á√ÉO:\n",
    "   - Melhor modelo (F1-Score): {melhor_modelo}\n",
    "   - Diferen√ßa em F1-Score: {abs(resultados_rf['f1'] - resultados_mlp['f1']):.4f}\n",
    "   - Ambos os modelos apresentaram performance {'similar' if abs(resultados_rf['f1'] - resultados_mlp['f1']) < 0.05 else 'diferente'}\n",
    "\n",
    "5. FEATURE IMPORTANCE (Random Forest):\n",
    "\"\"\"\n",
    "    \n",
    "    for idx, row in resultados_rf['feature_importance'].iterrows():\n",
    "        resumo += f\"   - {row['Feature']}: {row['Importance']:.4f}\\n\"\n",
    "    \n",
    "    resumo += f\"\"\"\n",
    "6. CONCLUS√ïES:\n",
    "   - {'Random Forest' if melhor_modelo == 'Random Forest' else 'MLP'} apresentou melhor performance geral\n",
    "   - As m√©tricas indicam que o modelo consegue classificar adequadamente as v√≠timas\n",
    "   - A valida√ß√£o cruzada mostra consist√™ncia nas predi√ß√µes\n",
    "   - O modelo pode ser utilizado para classificar novas v√≠timas baseado nos sinais vitais\n",
    "\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\"\"\"\n",
    "    \n",
    "    print(resumo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
